{"meta":{"title":"Learner","subtitle":"","description":"net core kibana blog 设计模式","author":"Leon Liu","url":"https://www.iwhero.com","root":"/"},"pages":[{"title":"","date":"2021-03-07T09:34:13.229Z","updated":"2021-03-07T09:34:13.224Z","comments":true,"path":"404.html","permalink":"https://www.iwhero.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2021-03-07T09:23:57.509Z","updated":"2021-03-07T09:23:57.509Z","comments":true,"path":"about/index.html","permalink":"https://www.iwhero.com/about/index.html","excerpt":"","text":"下面写关于自己的内容"},{"title":"所有分类","date":"2021-03-07T09:25:31.397Z","updated":"2021-03-07T09:25:31.391Z","comments":true,"path":"categories/index.html","permalink":"https://www.iwhero.com/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2021-03-07T09:29:50.475Z","updated":"2021-03-07T09:29:50.457Z","comments":true,"path":"friends/index.html","permalink":"https://www.iwhero.com/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"","date":"2021-03-07T09:28:33.166Z","updated":"2021-03-07T09:28:33.161Z","comments":true,"path":"mylist/index.html","permalink":"https://www.iwhero.com/mylist/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2021-03-07T09:27:16.562Z","updated":"2021-03-07T09:27:16.557Z","comments":true,"path":"tags/index.html","permalink":"https://www.iwhero.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":".Net Core单元测试","slug":"单元测试/NetCore单元测试","date":"2021-11-15T16:00:00.000Z","updated":"2021-11-16T07:06:37.616Z","comments":true,"path":"2021/11/16/单元测试/NetCore单元测试/","link":"","permalink":"https://www.iwhero.com/2021/11/16/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/NetCore%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","excerpt":"","text":"前言单元测试一直都是”好处大家都知道很多,但是因为种种原因没有实施起来”的一个老大难问题。具体是否应该落地单元测试，以及落地的程度, 每个项目都有自己的情况。 本篇为个人认为”如何更好地写单元测试”, 即更加偏向实践向中夹杂一些理论的分享。 下列示例的单元测试框架为xUnit, Mock库为Moq 为什么需要单元测试优点有很多, 这里提两点我个人认为的很明显的好处 防止回归回归缺陷是在对应用程序进行更改时引入的缺陷。 测试人员通常不仅测试新功能，还要测试预先存在的功能，以验证先前实现的功能是否仍按预期运行。 通常在进行新功能/模块的开发或者是重构的时候，测试会进行回归测试原有的已存在的功能，以验证以前实现的功能是否仍能按预期运行。使用单元测试，可在每次生成后，甚至在更改一行代码后重新运行整套测试, 从而可以很大程度减少回归缺陷。 比执行功能测试节省时间功能测试费用高。 它们通常涉及打开应用程序并执行一系列你（或其他人）必须遵循的步骤，以验证预期的行为。 测试人员可能并不总是了解这些步骤，这意味着为了执行测试，他们必须联系更熟悉该领域的人。 对于细微更改，测试本身可能需要几秒钟，对于较大更改，可能需要几分钟。 最后，在系统中所做的每项更改都必须重复此过程。 而单元测试只需按一下按钮即可运行，只需要几毫秒时间，且无需测试人员了解整个系统。 测试通过与否取决于测试运行程序，而非测试人员。 减少耦合代码当代码紧密耦合或者一个方法过长的时候，编写单元测试会变得很困难。当不去做单元测试的时候，可能代码的耦合不会给人感觉那么明显。为代码编写测试会自然地解耦代码，变相提高代码质量和可维护性 基本原则和规范3A原则3A分别是”arrange、act、assert”, 分别代表一个合格的单元测试方法的三个阶段 事先的准备 测试方法的实际调用 针对返回值的断言 一个单元测试方法可读性是编写测试时最重要的方面之一。在测试中分离这些操作会明确地突出显示调用代码所需的依赖项、调用代码的方式以及尝试断言的内容. 所以在进行单元测试的编写的时候, 请使用注释标记出3A的各个阶段的, 如下示例 12345678910111213141516/// &lt;summary&gt;/// 单位获取酒店政策/// &lt;/summary&gt;[Fact]public async Task GetHotelPolicy_Company()&#123; //arrange string cmpCode = &quot;xxx&quot;; //act List&lt;HotelPolicyDto&gt; hotelPolicyDtos = await _hotelPolicyServiceProxy.GetHotelPolicy(cmpCode); //assert _testOutputHelper.WriteLine(hotelPolicyDtos.ToJson()); Assert.True(hotelPolicyDtos.Count &gt; 0);&#125; 尽量避免直接测试私有方法尽管私有方法可以通过反射进行直接测试，但是在大多数情况下，不需要直接测试私有的private方法, 而是通过测试公共public方法来验证私有的private方法。 可以这样认为：private方法永远不会孤立存在。更应该关心的是调用private方法的public方法的最终结果。 避免多个断言如果一个测试方法存在多个断言，可能会出现某一个或几个断言失败导致整个方法失败。这样不能从根本上知道是了解测试失败的原因。 所以一般有两种解决方案 拆分成多个测试方法 使用参数化测试, 如下示例 1234567891011121314[Theory][InlineData(null)][InlineData(&quot;a&quot;)]public void Add_InputNullOrAlphabetic_ThrowsArgumentException(string input)&#123; // arrange var stringCalculator = new StringCalculator(); // act Action actual = () =&gt; stringCalculator.Add(input); // assert Assert.Throws&lt;ArgumentException&gt;(actual);&#125; 当然如果是对对象进行断言, 可能会对对象的多个属性都有断言。此为例外。 文件和方法命名规范文件名规范 一般有两种。比如针对UserController下方法的单元测试应该统一放在UserControllerTest或者UserController_Test下 单元测试方法名 单元测试的方法名应该具有可读性，让整个测试方法在不需要注释说明的情况下可以被读懂。格式应该类似遵守如下 12345678&lt;被测试方法全名&gt;_&lt;期望的结果&gt;_&lt;给予的条件&gt;&#x2F;&#x2F; 例子[Fact]public void Add_InputNullOrAlphabetic_ThrowsArgumentException()&#123; ...&#125; 常用类库介绍xUnit/MsTest/NUnit编写.Net Core的单元测试绕不过要选择一个单元测试的框架, 三大单元测试框架中 MsTest是微软官方出品的一个测试框架 NUnit没用过 xUnit是.Net Foundation下的一个开源项目，并且被dotnet github上很多仓库(包括runtime)使用的单元测试框架，Nunit的作者 客观地功能性地分析三大框架地差异可以参考如下 https://anarsolutions.com/automated-unit-testing-tools-comparison Moq官方仓库 https://github.com/moq/moq4 Moq是一个非常流行的模拟库, 只要有一个接口它就可以动态生成一个对象, 底层使用的是Castle的动态代理功能. 基本用法 在实际使用中可能会有如下场景 123456789101112131415161718192021222324public class UserController&#123; private readonly IUserService _userService; public UserController(IUserService userService) &#123; _userService &#x3D; userService; &#125; [HttpGet(&quot;&#123;id&#125;&quot;)] public IActionResult GetUser(int id) &#123; var user &#x3D; _userService.GetUser(id); if (user &#x3D;&#x3D; null) &#123; return NotFound(); &#125; else &#123; ... &#125; &#125;&#125; 在进行单元测试的时候, 可以使用Moq对_userService.GetUser进行模拟返回值 12345678910111213141516171819[Fact]public void GetUser_ShouldReturnNotFound_WhenCannotFoundUser()&#123; &#x2F;&#x2F; arrange &#x2F;&#x2F; 新建一个IUserService的mock对象 var mockUserService &#x3D; new Mock&lt;IUserService&gt;(); &#x2F;&#x2F; 使用moq对IUserService的GetUs方法进行mock: 当入参为233时返回null mockUserService .Setup(it &#x3D;&gt; it.GetUser(233)) .Return((User)null); var controller &#x3D; new UserController(mockUserService.Object); &#x2F;&#x2F; act var actual &#x3D; controller.GetUser(233) as NotFoundResult; &#x2F;&#x2F; assert &#x2F;&#x2F; 验证调用过userService的GetUser方法一次，且入参为233 mockUserService.Verify(it &#x3D;&gt; it.GetUser(233), Times.AtMostOnce());&#125; Xunit.DependencyInjection基于xunit和微软依赖注入框架的“真正”的依赖注入使用方式 ——— Xunit.DependencyInjection, 来自大师的作品，让你在测试代码里使用依赖注入像 asp.net core 一样轻松 具体使用 基于Xunit.DependencyInjection在Xunit中使用依赖注入 实践中结合Vistial Studio使用Visual Studio提供了完备的单元测试的支持，包括运行. 编写. 调试单元测试。以及查看单元测试覆盖率等。 如何在Visual Studio中查看单元测试覆盖率如下功能需要Visual Studio 2019 Enterprise版本，社区版不带这个功能。 如何查看覆盖率 在测试窗口下，右键相应的测试组 点击如下的”分析代码覆盖率” 扩展TDD介绍TDD是测试驱动开发（Test-Driven Development）的英文简称. 一般是先提前设计好单元测试的各种场景再进行真实业务代码的编写，编织安全网以便将Bug扼杀在在摇篮状态。 此种开发模式以测试先行，对开发团队的要求较高, 落地可能会存在很多实际困难。详细说明可以参考如下 https://www.guru99.com/test-driven-development.html","categories":[{"name":"xunit","slug":"xunit","permalink":"https://www.iwhero.com/categories/xunit/"}],"tags":[{"name":"xunit","slug":"xunit","permalink":"https://www.iwhero.com/tags/xunit/"}]},{"title":"基于Xunit.DependencyInjection在Xunit中使用依赖注入","slug":"单元测试/更优雅的在-Xunit-中使用依赖注入","date":"2021-10-25T16:00:00.000Z","updated":"2021-11-16T07:04:37.572Z","comments":true,"path":"2021/10/26/单元测试/更优雅的在-Xunit-中使用依赖注入/","link":"","permalink":"https://www.iwhero.com/2021/10/26/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/%E6%9B%B4%E4%BC%98%E9%9B%85%E7%9A%84%E5%9C%A8-Xunit-%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/","excerpt":"","text":"简介https://www.cnblogs.com/weihanli/p/xuint-dependency-injection.html ，最近大师完成了 7.0 的重构并且已经正式发布，已经可以直接安装使用了 7.0 为我们带来了更好的编程体验，在 6.x 的版本中，我们的 Startup 需要继承于 DependencyInjectionTestFramework 而且需要设置一个 assembly attribute，这在 7.0 中都不需要了，下面我们来看看有了哪些变化 Startup 的变化123456789101112131415161718192021222324-[assembly: TestFramework(&quot;Your.Test.Project.Startup&quot;, &quot;Your.Test.Project&quot;)]namespace Your.Test.Project&#123;- public class Startup : DependencyInjectionTestFramework+ public class Startup &#123;- public Startup(IMessageSink messageSink) : base(messageSink) &#123; &#125;- protected void ConfigureServices(IServiceCollection services)+ public void ConfigureServices(IServiceCollection services) &#123; services.AddTransient&lt;IDependency, DependencyClass&gt;(); &#125;- protected override IHostBuilder CreateHostBuilder() &#x3D;&gt;- base.CreateHostBuilder(assemblyName)- .ConfigureServices(ConfigureServices);- protected override void Configure(IServiceProvider provider)+ public void Configure(IServiceProvider provider) &#125;&#125; 移除了 TestFramework assembly attribute 不再需要继承于 DependencyInjectionTestFramework 也因为上面的不需要继承，所以原本要 override 的方法可以不 override 了，原来是 protected 的方法现在需要改成 public这样改了之后首先我们在使用的时候无需知道 DependencyInjectionTestFramework 的存在了，而且可以更符合 asp.net core Startup 的使用习惯，可以屏蔽掉很多实现细节，用户只需要在 Startup 注册自己的逻辑即可，更为专注于自己的逻辑而无需关心框架所做的事情 新的 Startup 解析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182namespace XX.UnitTest&#123; public class Startup &#123; &#x2F;&#x2F; 自定义 HostBuilder ，可以没有这个方法，没有这个方法会使用默认的 hostBuilder，通常直接使用 &#96;ConfigureHost&#96; 应该就够用了 &#x2F;&#x2F; public IHostBuilder CreateHostBuilder() &#x2F;&#x2F; &#123; &#x2F;&#x2F; return new HostBuilder() &#x2F;&#x2F; .ConfigureAppConfiguration(builder &#x3D;&gt; &#x2F;&#x2F; &#123; &#x2F;&#x2F; &#x2F;&#x2F; 注册配置 &#x2F;&#x2F; builder &#x2F;&#x2F; .AddInMemoryCollection(new Dictionary&lt;string, string&gt;() &#x2F;&#x2F; &#123; &#x2F;&#x2F; &#123;&quot;UserName&quot;, &quot;Alice&quot;&#125; &#x2F;&#x2F; &#125;) &#x2F;&#x2F; .AddJsonFile(&quot;appsettings.json&quot;) &#x2F;&#x2F; ; &#x2F;&#x2F; &#125;) &#x2F;&#x2F; .ConfigureServices((context, services) &#x3D;&gt; &#x2F;&#x2F; &#123; &#x2F;&#x2F; &#x2F;&#x2F; 注册自定义服务 &#x2F;&#x2F; services.AddSingleton&lt;IIdGenerator, GuidIdGenerator&gt;(); &#x2F;&#x2F; if (context.Configuration.GetAppSetting&lt;bool&gt;(&quot;XxxEnabled&quot;)) &#x2F;&#x2F; &#123; &#x2F;&#x2F; services.AddSingleton&lt;IUserIdProvider, EnvironmentUserIdProvider&gt;(); &#x2F;&#x2F; &#125; &#x2F;&#x2F; &#125;) &#x2F;&#x2F; ; &#x2F;&#x2F; &#125; &#x2F;&#x2F; 自定义 host 构建 public void ConfigureHost(IHostBuilder hostBuilder) &#123; hostBuilder .ConfigureAppConfiguration(builder &#x3D;&gt; &#123; &#x2F;&#x2F; 注册配置 builder.AddJsonFile(&quot;appsettings.Development.json&quot;); &#125;) .ConfigureServices((context, services) &#x3D;&gt; &#123; &#x2F;&#x2F; 注册自定义服务 &#x2F;&#x2F;services.AddSingleton&lt;IIdGenerator, GuidIdGenerator&gt;(); &#125;); &#125; &#x2F;&#x2F; 支持的形式： &#x2F;&#x2F; ConfigureServices(IServiceCollection services) &#x2F;&#x2F; ConfigureServices(IServiceCollection services, HostBuilderContext hostBuilderContext) &#x2F;&#x2F; ConfigureServices(HostBuilderContext hostBuilderContext, IServiceCollection services) public void ConfigureServices(IServiceCollection services, HostBuilderContext hostBuilderContext) &#123; var configuration &#x3D; hostBuilderContext.Configuration; services.AddSingleton(new Appsettings(hostBuilderContext.HostingEnvironment.ContentRootPath, &quot;Development&quot;)); services.AddHttpClient&lt;IGeneralHttpClient, GeneralHttpClient&gt;(); services.AddConfigOptionSetup(configuration); services.AddServicesSetup(); services.AddRedisSetup(); services.AddDbContextSetup(configuration); services.AddElasticSearchSetup(configuration); services.AddTimedServiceSetup(); &#125; &#x2F;&#x2F; 可以添加要用到的方法参数，会自动从注册的服务中获取服务实例，类似于 asp.net core 里 Configure 方法 public void Configure(IServiceProvider applicationServices) &#123; &#x2F;&#x2F; 有一些测试数据要初始化可以放在这里 &#x2F;&#x2F; InitData(); &#125; &#125;&#125; 在新的版本中 Startup 和 asp.net core 里的 Startup 更加相像了， 会多一个 CreateHostBuilder/ConfigureHost(IHostBuilder) 的方法，允许用户自定义 Host 的构建，也可以没有这个方法 ConfigureServices 方法允许用户增加 HostBuilderContext 作为参数，可以通过 hostBuilderContext 来获取配置信息，也可以在 CreateHostBuilder/ConfigureHost(IHostBuilder) 里注册也是一样的 注册配置/服务和 asp.net core 里一模一样，有数据或配置需要在项目启动时初始化的，可以放在 Configure 方法做，有点类似于 asp.net core 里 Startup 中的 Configure 方法，可以将需要的服务作为方法参数，执行时会自动从注册的服务中获取 单元测试1234567891011121314151617181920212223242526272829namespace XX.UnitTest&#123; public class ApiTest &#123; private readonly ISupplierService _supplierService; public ApiTest(ISupplierService supplierService) &#123; _supplierService &#x3D; supplierService; &#125; [Fact] public void HotelListLowPriceRequest() &#123; HotelListLowPriceRequest request &#x3D; new HotelListLowPriceRequest() &#123; SearchTypeEntity &#x3D; new SearchTypeEntity() &#123; SearchType &#x3D; &quot;WirelessSearch&quot;, HotelCount &#x3D; 100, PageIndex &#x3D; 1 &#125; &#125;; var res &#x3D; _supplierService.GetHotelListLowPrice(request); &#125; &#125;&#125; Startup 的寻找方法默认的 Startup 通常是 ProjectName.Startup，通常在项目根目录下创建一个 Startup 是不需要配置的，如果不是或不起作用，可以参考下面 Startup 的寻找规则 如果要使用一个特别的 Startup, 你可以通过在项目文件的 PropertyGroup 部分定义 XunitStartupAssembly 和 XunitStartupFullName，具体规则如下 12345678&lt;Project&gt; &lt;PropertyGroup&gt; &lt;XunitStartupAssembly&gt;a&lt;&#x2F;XunitStartupAssembly&gt; &lt;XunitStartupFullName&gt;b&lt;&#x2F;XunitStartupFullName&gt; &lt;&#x2F;PropertyGroup&gt;&lt;&#x2F;Project&gt; 赶紧试试吧~~ Reference https://github.com/pengweiqhca/Xunit.DependencyInjection https://github.com/WeihanLi/SamplesInPractice/tree/master/XUnitDependencyInjectionSample","categories":[{"name":"xunit","slug":"xunit","permalink":"https://www.iwhero.com/categories/xunit/"}],"tags":[{"name":"xunit","slug":"xunit","permalink":"https://www.iwhero.com/tags/xunit/"}]},{"title":"Redis笔记","slug":"学习/redis笔记","date":"2021-08-31T16:00:00.000Z","updated":"2021-11-16T07:06:19.340Z","comments":true,"path":"2021/09/01/学习/redis笔记/","link":"","permalink":"https://www.iwhero.com/2021/09/01/%E5%AD%A6%E4%B9%A0/redis%E7%AC%94%E8%AE%B0/","excerpt":"","text":"reids :数据库，，存储数据的仓库，它是内存数据库，然后提供了好多种数据类型，来解决我们不同业务的问题。。 nosql数据库的一种数据库，如果做缓存，优先选择redis。。 1.为什么在大数据高并发业务中，会要有redis？ IO阻塞问题。。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283String 这是一个简单的键值对操作。。 redis 本身是一个内存数据，内存中的数据越来越多的时候，内存不是不够用了吗。可以给数据设置过期时间。。 过期时间设置-1，是过多久过期，2是可以直接设置具体的日期 使用场景： 1.session 利用redis做session共享内存 2.自增和自减法 -- 做一些网站的请求数量，或者论坛的点赞数，评论数，不可能每次都去执行数据库。。。 可以直接利用redis，操作内存，只不过到最后，做个数据刷盘，把这些统计数据放到我们硬盘中。。。 string 底层 ： 在功能中，出了非必要的情况，除了上述这几个需求，尽量不要使用string类型 底层会浪费大量的内存空间 如果使用raw编码，则每次开辟空间都会留一些空间，如果数据长度变了，则内存也会继续变大。。 如果你使用embstr ：它每次最多开辟64个字节的空间，只有44个字节时存储我们数据的。 如果你在操作的redis的时候，内容长度小于等于44，则会自动选择embstr编码开辟空间 如果你操作redis的时候，内容长度大于44的，使用ram编码，浪费 空间 还有一个int:只是针对于写的数据是数值，才行。。切记只有整型才是int类型 底层就是因为开辟的组件的原因。。。所以会浪费空间，尽量不要使用string。。除非你不在乎那些内存，就这样搞。。。 为什么要这样设置，给你一种选择。。。 Hash官网推荐使用hash 。。。hash最典型的使用场景。。 储存一个用户信息 id,name,address,email 如果使用string去存储，key=userid values=&#123;id:1,name:&#x27;&#x27;,address:&#x27;&#x27; ====&#125;如果需要修改其中木一个值，先把数据拿到代码内存，然后反序列化，然后修改值，然后序列化--存储到redis 如果直接使用hash，则值需要制定修改具体某一个字段的值就可以了 hash 底层，ziplist,hash ?ziplist:压缩版的list ..(动态的数组，数组的每一个元素的空间是一样的)第一问题：每次插入都有开辟空间，连续的第二问题：你要查询的时候，你要从头来计算，--查询的速度变慢。。。hash的数据结果，它的时间复杂度是O(1);hash快速的查询结果，而且节省空间，一上来就是一个小量的hash。。问题：如果hash后面链表越来越长的时候，时间复杂度是不是又变高。。。解决了这个问题？？扩容：是一开始的时候，redis有两个hash结构在存储数据，第一次只要一个是有长度，一个是没有长度。。数据迁移问题来了，因为如果不小心操作的时候，刚好触发了瓶颈，要扩容，解决迁移问题： 第一个：不是一次性迁移完成，是每一次操作只会迁移一部分。 第二个：是一个后台任务，后台任务给你偷偷摸摸的迁移数据： 非常重要：数据一致的问题：：自己有机会了解一下 针对于hash get到的扣个1：不管是从内存还是性能方面，都是比较牛逼，所以官网推荐使用hash。。。扩容是有阈值。不会随便扩的。。 List: 第一个是队列（先进先出） 第二个是栈：（先进后出） 第三个就是普通的集合。 如果你要做插队，或者做队列，都可以使用list 集合设置过期，只能给整个集合设置，不能单独给某一个元素设置，没有给单独元素设置过期时间的策略。。 遇到分页场景也可以使用。。。 消息队列： 记录日志 程序报错--- 一种是文本日志，一种是数据库日志，，。 可以通过底层的消息队列来实现 如果不想用list做消息队列，也可以直接用提供的专门的消息发布来做。。 特殊情况，公司不想要新的技术栈，就想用redis， -- 提供一个扩展，不想了解也没有关系，知道这么回事就可以了 Set：也是一个集合，只不过是一个去重的集合，比如，比如我需要做投票-- 根据ip地址来投票，每一个ip只投一票。。。如果用list，需要我们自己来判断。如果使用set，则系统自动会去重。。。 取交集和去并集a:&#123;1,2,3,4,5&#125;b：&#123;2,3,4,5,6&#125;a和b的交集= &#123;2,3,4,5&#125;a和b的并集=&#123;1,2,3,4,5,6&#125;qq好友推荐里面用的比较多Zset：也是去重的集合，具有set的功能，而且在这些功能里面加了一个值，分数。。。使用场景，做服务注册于发现，抽奖。。。限流。。排行榜 。。。。 提供了一个非常强大的功能，自动排序数据结构底层：跳跃表 利用跳跃表，解决排序问题，把内容存在hash里面，把数值存在跳跃表里面，跳跃表里面的里面的数据是有顺序的。。。 跳跃表在存放数据的时候，就把顺序搞好了。。。 -- 它不是解决存放的问题，它是损失一部分写的性能，来提升查询的性能 跳跃表就相当于我们leven树。 一层上面有一层的大纲，后面我讲性能调优模块的时候，会单独讲数据结构和算法，给你提供.net跳跃表的源码 。。。。 zset默认是顺序查询，--我们默认查询是先根据分数来查询我们数据，我们的业务是尽量使用分数来查询，---有时间，可以听听上一期-- 课次数没有变--内容增多，希望大家，集合上一期。。api ,这一期api比较少，主要是原理-- BitMapsHyperloglossStreamsBloom Filter数据结构 1234c# Socket.Select()Socket.Poll()的静态函数是多路复用吗Windows平台的重叠IO概念就是多路复用吗select 模型多路复用，性能还可以，但是没有linux,epool生产环境中建议安装在linux系统中。。 1234561、window 环境下有进程的内存大小限制吧？redis在windows下最大可管理多少内存可以配置的额，最后一次课讲解数据淘汰算法的时候，会讲到，怎么给redis设置最大内存限制。。2、Exchange和CSRedisCore还有ServiceStack是不是API都一样（方法名称不一样，功能一样），只是性能不一样？不一样，，ServiceStack是目前在生产环境中用的最多的，只不过破解，或者收费-- Exchange3、数据淘汰机制如何设置 123456问题：1.windows上面生产环境跑5.0.10版本的redis的主要弊端是什么？不能利于多路复用的epool模型。。。 只能使用select模型，最大能并发能支持1024个请求连接2.hash里面的key可以存一个对象吗？api里面存一个对象里面套对象会怎么样？redis里面的values只要是数数都可以写进去，因为写进去之前服务会做序列化开发中，要么就是一个key，要么是string，要么就是json序列化的数组 123456789101112131415Redis 安装到windows下是不是也不能多路复用mysql在linux里面是不是也能实现IO多路复用（）ngixn+redis 是用了多路复用1.list 是否支持key 搜索 。。没懂这个问题1 redis 设置了密码客户端加密码是这么加吗using (RedisClient client = new RedisClient(&quot;password@127.0.0.1&quot;，6379))2redis 的list 和我们泛型list 是不是区别不大？不一样，底层不一样，.net 泛型list是一个安全类型的动态数组，redis里面的list代表的是一个类型，而不是数据结构，数据结构底层是之前的版本是双向链表，新版面是quiltlist(链表+ziplist)3 今天 select和epllo没有讲清楚只讲了 多路复用io 非阻塞 如果想要在java上面执行那个多路复用，如果拿到代码不会，运行不起来--- 少数人---老师level是怎么分级的啊，什么时候分下一级啊跳跃表：通过随机数，来要不要上升一级，redis跳跃表，底层代码最多是32级。。 特别感兴趣的话，和我要一下源代码（。net跳跃表的代码，如果不着急，后面讲数据结构和算法的时候，还会单独去讲 ）mysql8 是多线程分布式关系型数据库tidb (多线程) redis 解决了两个问题：1.直接操作硬盘的问题 ​2.连接阻塞的问题，之前是io阻塞，系统在等待请求的时候，不能干其他的事情，而且每次只能阻塞等待请求，redis通过多路复用，由系统帮你监听多个连接，只要有事情要处理 ，系统会调用你的回调函数，把监听的事情交给了系统，释放了我们自己的代码– 开启的是高速通道，之前一次只能处理一个请求，现在可以处理多个请求。。。 （不管是单线程的redis还是多线程的redis，最后执行那个指令的线程并且只要一个，） 123456789101112131.内存满了，会触发数据淘汰策略，释放资源2.限流--讲lua，扩展，看一下实现代码 ### 限流lua代码如下local times = redis.call(&#x27;incr&#x27;,KEYS[1])if times == 1 then redis.call(&#x27;expire&#x27;,KEYS[1], ARGV[1])end if times &gt; tonumber(ARGV[2]) then return 0endreturn 1 1redis-cli --eval limit.lua rate.limit:127.0.0.1 , 5 6 1234567891011121314151617181920212223242526272829/// &lt;summary&gt;/// 滑窗式限流 利用redis的Zset类型/// &lt;/summary&gt;/// &lt;returns&gt;&lt;/returns&gt;public bool LimitFlow()&#123; DateTime currentTime = DateTime.Now; using (IRedisClient client = redisService.GetRedisClient()) &#123; // 判断是否存在响应的key if (client.ContainsKey(&quot;服务名称&quot;)) &#123; //获取前一秒和当前秒时间类的集合总数量 //此数量表示的就是请求数量 int count = client.GetRangeFromSortedSetByLowestScore(&quot;服务名称&quot;， currentTime.AddSeconds(-1).ToFileTimeUtc()， currentTime.ToFileTimeUtc()).Count; // 8表示的是限流的数量 //如果请求数量大于限流数量，则返回false，可以拒绝请求 if (count != null &amp;&amp; count &gt;8) &#123; return false; &#125; &#125; //如果是第一次获取请求数量 //或者满足限流限制，则返回true，然后给Zset中添加一次请求数量，分数就是当前的时间戳 client.AddItemToSortedSet(&quot;服务名称&quot;， Guid.NewGuid().ToString()， currentTime.ToFileTimeUtc()); return true; &#125;&#125; 生产环境，都不支持key,因为消耗性能比较严重。。。内存满了之后会触发数据淘汰算法，可能会丢数据，但是不会让数据库宕机— 可以配置 还有20个字节存放了其他的标识—– 自己百度查查， zset只能根据分数来查询。。。 redis。ngixn都是使用的多路复用，高并发场景— 如果网络协议，socket, dotnetty —.net 版本的 netty框架 1https://github.com/Azure/DotNetty 多路复用底层是系统提供 .net 里面还没有去使用linux系统里面的epool模型，可以使用select，redis可以做，c语言写但是.net 还没有调用这种东西。。。 Docker 中如果设置redis密码 挂载的时候，把配置文件改一下，挂载进去就行了。或者直接进容器改配置文件— 等下，把docker挂载启动redis配置和脚本放在下面 ：： 1docker run -d -p 6379:6379 -v /myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf --name redis01 redis:6.0 redis-server /usr/local/etc/redis/redis.conf 1配置文件就是windows下面的配置文件就可以，然后存放到/myredis/conf/redis.conf 这个目录，就可以使用docker挂载了，然后可以设置密码什么的 123456789101112131415161718192021222324BitMaps是一个节省内存的数据结构。。0,1...Hyperlogloss:基准统计数据结构类型投票选举--- 需要大量的投票的人（14亿*3）--候选人：100使用少量的内存，存放大量的数据。。。统计的结果有误差。。网页浏览统计，访问量等等这些不是特别需要准确的数据，可以利用它。Streams：发布订阅--- kafak的生产者和消费者的模型都是仿照它做的。业务用的上，内部组件，用的比较多。。Geo:是一个做地理位置，不同key存储的就是一个经度和纬度。可以计算俩个key之间的距离。。Bloom Filter数据结构 ---布隆过滤器过滤器：1.存储数据----占用很少的内存存储跟多的数据。。2.判断数据在不在存储的里面 ---存在一个查询效率的问题优点：完美的解决上面这两个问题。。bit === 8个位==1一个字节--- 建立一个数据结构--bit[] bit数组通过布隆过滤器判断key在不在里面，有一个误判的问题。。。bit数组越长，hash的方法越多，则占用内存越多，但是误判的概率就小。bit数组越短，hash方法越少，则占用的内存越少，但是误判的概率就大。。。使用场景和解决什么--下一次课，讲常见面试题和实践问题的时候，在画架构图。。 12345678910111213141516171819ACID保证执行多个指令的时候，要么都执行，要么都不执行，要么都成功，要么都没有成功Redis事务：提交--没有回滚，提交了就提交，成功或者不成功，只会返回一个结果。redis里面使用普通事务，没有锁===建议：如果要使用redis的事务，则需要和Watch方法一起使用，而且最好，监听的这些字段都包含在所有的key里面。。小菜鸟玩事务，大神搞luaRedis ,直接支持lua脚本整个lua脚本的代码，本身就具有原子性。。多线程--- 并发的问题。。可以把lua理解成数据存储过程，但是实质不一样，而且lua具有原子性，而且性能要比我们直接操作指令性能要高---lua是c语言编写的，redis也是c语言编写，不存在编译过程--- 只是简单的举了一个案例 12345快照的方式（RDB）---每次都去抓拍一下全面貌 每次都全量备份-- 文件追加方式（AOF）--在之前的基础上追加 手动备份策略： 1.save: 阻塞线程— 因为执行这个指令的线程就是redis里面唯一的那个执行指令的线程- 当你备份大量的数据的时候，如果耗时比较长，则当有其他客户端发送指令的时候，会卡主 2.bgsave： 单独的线程，后台专门有一个线程去实现备份。 这个后台的就是专门来根据我们的配置文件里策略然后去备份数据文件 底层原理： 有一个定时器，就是不停的计算我们阈值。每一次bgsave之后，要把计数器清0=== 优势：备份缓慢，但是重启加载数据性能比较快。。。 日志追加：基本都是顺序读写//本身要比我们操作关系型数据库要快（mysql==） 优点是：文件可读行高，每次都是追加修改的东西，备份比较快，但是当服务重启的时候，加载数据的时候比较慢—素描 —- 如果rdb和aof都开启了，则默认首先加载aof文件，为了保证数据的尽可能完整性。。 aof 重写问题：当数据文件到达一个阈值的时候，，我们的文件会继续重写变成一个小文件。。 rdb:备份慢，启动快 aof:备份快，启动慢 redis..4X 版本之后，混合模式— 混合模式【备份，启动快】 备份的时候，我通过aof备份，我还原的时候根据rdb… 架构师的思想理念–怎么设计—（后台有一个任务，专门负责吧aof文件转化成rdb文件） aof–文件重写的问题—到数据大的时候， 当我们进行aof操作的时候，把aof文件变成rdb，然后之后的操作直接取做日志追加。。 默认的情况下，你要你开启了aof则，混合默认自动打开。 混合模式的文件–包含aof和rdb; 彻彻底底搞明白，听懂的把666刷起来 如果要redis的性能非常高，就不要持久化 如果要保证数据的完整性，要根据自己的业务来选择不同的持久化策略，一般情况都是使用混合模式 第一次配置的时候，切记吧aof打开（默认开启混合模式） docker run -d -p 5001:6379 –name redis1 redis 主 docker run -d -p 5002:6379 –name redis2 redis 从 docker run -d -p 5003:6379 –name redis3 redis 从 docker exec -it redis1 /bin/bash docker exec -it redis2 /bin/bash docker exec -it redis3 /bin/bash 主从脚本和环境没有问题… 主从是实现读写分离，然后主节点会把数据同步到从节点里面 用法是实现高可用，主节点宕机了，从节点可以替换主节点，不是自动替换，需要程序员修改代码，改ip地址和端口，或者应该是用keeplive 用vip地址来高可用。。。主从是不会自动切换–如果要自动切换要使用哨兵。。。 注意点：一般情况下，我们使用主从的时候，防止数据不一致，从节点只能读，不让他写， 默认从节点不会进行写操作。。。 所以当主节点宕机了，我们需要改代码，收到去修改redis的配置。。 redis多线程和线程的区别–redis 6.X 功能点比较全— 哨兵哨兵，最少是三台服务器，必须是单数（投票防止平票的情况）— 宕机情况： 1.主观宕机：单独哨兵认为你宕机了，发现了故障。。 2.客观宕机：半数哨兵认为主节点宕机，发现了故障。 选举主节点的原则： 1.健康度。。从节点响应的时间 2.完整性。根据我们从节点备份的完整性，根据数据备份偏移量 3.稳定性。根据启动时间周期，心跳检测 4.如果上面三个条件都相等，则根据我们节点启动时分配的run id来，如果runid 越小，则最有可能选择为我们主节点 1.主节点宕机，从节点替换主节点，然后过一会主节点启动了，则我们的主变成从节点。。 哨兵或者主从里面的数据问题： 1.脑裂问题 出现了主节点和哨兵之间网络原因，而且有多数以上的哨兵认为主节点宕机，则再从会从节点现在一个主，这个时候客户端代码还是可以连接到之前的主节点的，可以写数据，此时哨兵选举了新的主节点，然后之前的主网络恢复了，然后之前的主节点备份现在的主节点数据，造成数据不完整。。。 2.异步复制数据丢失问题 因为是异步复制数据，如果主节点和从节点直接数据复制太慢，在这之间主节点宕机，而且是真的宕机，这个时候从节点替换主节点，丢失了数据。。 哨兵–不管怎么样的配置都没有办法保证数据百分之白不丢失，只能尽可能少量丢数据 怎么解决上面这来个问题呢？ 需要改配置文件： 1.至少有几个从节点。配置=0，代表的是，当主节点和从节点之间互通的时候，发现从节点小于一个的时候，则从节点不会再继续给客户端提供服务。。 解决脑裂问题。。 2.偏移量配置。。。主节点和从节点数据之前偏移量只差，如果偏移量只差比配置小，则主节点也不会提供服务。。 第一个配置是主节点最少能连接到几个从节点 第二配置是主节点和从节点的ack时间差-响应（数据复制完整性问题，偏移量） 集群模式：多个节点提共服务 1.副本集的集群— 多个节点都可以读都可以写，而且多个节点之间数据是需要一致的。有大量的数据冗余。。 kafka… tidb 数据一致性问题需要解决— 每个节点都负责所有的数据 2.分片模式。每一个节点只负责一部分的数据写。 redis采用的集群模式是用的分片模式– 1.数据是否分配均匀 2.数据节点增删对数据分布影响不能太大。性能问题 redis集群数据分配规则–数据槽 哈希取余： 比如3个节点。。。 hash(key)%3= 0,1,2 1,2,3,4,5,6,7,8,9==3个节点 1,2,0,1,2,0,1,2,0 1,2,3,4,5,6,7,8,9==2个节点 1,2,0,1,2,0,1,2,0 1,0,1,0,1,0,1,0,1==90%的数据需要迁移-需要大量的时间— 计算分配那个节点，把数据传输到具体的节点 一致性哈希：哈希环–通过哈希环的方式，来解决数据大量迁移的问题 先哈希，比如结果=1，则把数据存放在哈希环上面1-2直接的虚拟节点上面 上面这些分配，都有依赖— 相当于把– 分槽：尽量让槽多一些—，把数据分配到指定的16384个槽上面去 取余根据16383取余。。写死就这么多篮子，不要和数据节点有依赖 集群优化细节： 集群要把数据都分配完成。。集群状态发送了改变，然后数据要迁移。。 如果数据迁移时间长–整个服务不能使用了， 表示：槽全部分配完可以，使用，或者槽没有分配完也可以使用，这个就根据公司自己业务 cluster-require-full-coverage ： no 没有完全分配就提供服务，yes，必须是全部搞完才分配 集群要注意的点： 集群是做了分片： 集群中事务支持吗，集群中lua支持吗？？？ 如果事务或者lua涉及到key在多个节点里面，都不支持，同样还有一些其他指令也是不支持。 集群里面数据库只要一个db，单机是有16个db 就目前我知道大厂，用生产环境中用集群不多，老项目— 如果是新项目，不要使用单机，也不要使用哨兵，官方推荐使用集群，而且官方测试，集群节点最好不要大于1000个节点。。。 可以提问：： redis6 新版本，代理连接，连接一个地址–不敢用，只是一个实验—不敢上生产 默认情况，当一个分片里面主节点和从节点全部宕机，则认为整个集群不可用了，就不会提供服务 环境就是集群，但是就想操作多个key，但是不知道这个key会被分配到那个节点上面去、、 {aa}–如果key里面有括弧，则表示哈希的时候就根据括弧里面的内容来哈希 比如扩容，先启动节点，然后把节点加入到集群里面，然后把其他节点的数据分配一些过来，可以选择分配个节点，多少数据 缩容，先把要删除的节点的数据迁移到其他节点，然后把这个节点在从集群移除 –上一期的视频里面。扩容和缩容 集群优化，redis优化，如果是linux，根据不同版本，把同时能打开的文件句柄配置最大","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"https://www.iwhero.com/tags/redis/"}]},{"title":"状态模式详解","slug":"设计模式/状态模式","date":"2021-08-27T16:00:00.000Z","updated":"2021-11-16T07:06:03.493Z","comments":true,"path":"2021/08/28/设计模式/状态模式/","link":"","permalink":"https://www.iwhero.com/2021/08/28/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"定义：(源于Design Pattern)：当一个对象的内在状态改变时允许改变其行为，这个对象看起来像是改变了其类。 上述是百度百科中对状态模式的定义，定义很简单，只有一句话，请各位形象的去理解这句话，它说当状态改变时，这个对象的行为也会变，而看起来就像是这个类改变了一样。 这正是应验了我们那句话，有些人一旦发生过什么事以后，就像变了个人似的，这句话其实与状态模式有异曲同工之妙。 我们仔细体会一下定义当中的要点。 有一个对象，它是有状态的。 这个对象在状态不同的时候，行为不一样。 这些状态是可以切换的，而非毫无关系。 前两点比较好理解，第3点有时候容易给人比较迷惑的感觉，什么叫这些状态是可以切换的，而非毫无关系？ 举个例子，比如一个人的状态，可以有很多，像生病和健康，这是两个状态，这是有关系并且可以转换的两个状态。再比如，睡觉、上班、休息，这也算是一组状态，这三个状态也是有关系的并且可以互相转换。 那如果把生病和休息这两个状态放在一起，就显得毫无意义了。所以这些状态应该是一组相关并且可互相切换的状态。 下面我们来看看状态模式的类图。 类图中包含三个角色 Context：它就是那个含有状态的对象，它可以处理一些请求，这些请求最终产生的响应会与状态相关。 State：状态接口，它定义了每一个状态的行为集合，这些行为会在Context中得以使用。 ConcreteState：具体状态，实现相关行为的具体状态类。 如果针对刚才对于人的状态的例子来分析，那么人（Person）就是Context，状态接口依然是状态接口，而具体的状态类，则可以是睡觉，上班，休息，这一系列状态。 我们来试着写一个DOTA的例子，最近貌似跟DOTA干上了，不为其他，就因为DOTA伴随了LZ四年的大学时光。 玩过的朋友都知道，DOTA里的英雄有很多状态，比如正常，眩晕，加速，减速等等。相信就算没有玩过DOTA的朋友们，在其它游戏里也能见到类似的情况。那么假设我们的DOTA没有使用状态模式，则我们的英雄类会非常复杂和难以维护，我们来看下，原始版的英雄类是怎样的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.state;//英雄类public class Hero &#123; public static final int COMMON = 1;//正常状态 public static final int SPEED_UP = 2;//加速状态 public static final int SPEED_DOWN = 3;//减速状态 public static final int SWIM = 4;//眩晕状态 private int state = COMMON;//默认是正常状态 private Thread runThread;//跑动线程 //设置状态 public void setState(int state) &#123; this.state = state; &#125; //停止跑动 public void stopRun() &#123; if (isRunning()) runThread.interrupt(); System.out.println(&quot;--------------停止跑动---------------&quot;); &#125; //开始跑动 public void startRun() &#123; if (isRunning()) &#123; return; &#125; final Hero hero = this; runThread = new Thread(new Runnable() &#123; public void run() &#123; while (!runThread.isInterrupted()) &#123; try &#123; hero.run(); &#125; catch (InterruptedException e) &#123; break; &#125; &#125; &#125; &#125;); System.out.println(&quot;--------------开始跑动---------------&quot;); runThread.start(); &#125; private boolean isRunning()&#123; return runThread != null &amp;&amp; !runThread.isInterrupted(); &#125; //英雄类开始奔跑 private void run() throws InterruptedException&#123; if (state == SPEED_UP) &#123; System.out.println(&quot;--------------加速跑动---------------&quot;); Thread.sleep(4000);//假设加速持续4秒 state = COMMON; System.out.println(&quot;------加速状态结束，变为正常状态------&quot;); &#125;else if (state == SPEED_DOWN) &#123; System.out.println(&quot;--------------减速跑动---------------&quot;); Thread.sleep(4000);//假设减速持续4秒 state = COMMON; System.out.println(&quot;------减速状态结束，变为正常状态------&quot;); &#125;else if (state == SWIM) &#123; System.out.println(&quot;--------------不能跑动---------------&quot;); Thread.sleep(2000);//假设眩晕持续2秒 state = COMMON; System.out.println(&quot;------眩晕状态结束，变为正常状态------&quot;); &#125;else &#123; //正常跑动则不打印内容，否则会刷屏 &#125; &#125;&#125; 下面我们写一个客户端类，去试图让英雄在各种状态下奔跑一下。 123456789101112131415161718package com.state;public class Main &#123; public static void main(String[] args) throws InterruptedException &#123; Hero hero = new Hero(); hero.startRun(); hero.setState(Hero.SPEED_UP); Thread.sleep(5000); hero.setState(Hero.SPEED_DOWN); Thread.sleep(5000); hero.setState(Hero.SWIM); Thread.sleep(5000); hero.stopRun(); &#125;&#125; 可以看到，我们的英雄在跑动过程中随着状态的改变，会以不同的状态进行跑动。 在上面原始的例子当中，我们的英雄类当中有明显的if else结构，我们再来看看百度百科中状态模式所解决的问题的描述。 状态模式解决的问题：状态模式主要解决的是当控制一个对象状态的条件表达式过于复杂时的情况。把状态的判断逻辑转移到表示不同状态的一系列类中，可以把复杂的判断逻辑简化。 不用说，状态模式是可以解决我们上面的if else结构的，我们采用状态模式，利用多态的特性可以消除掉if else结构。这样所带来的好处就是可以大大的增加程序的可维护性与扩展性。 下面我们就使用状态模式对上面的例子进行改善，首先第一步，就是我们需要定义一个状态接口，这个接口就只有一个方法，就是run。 123456789package com.state;public interface RunState &#123; void run(Hero hero); &#125; 与状态模式类图不同的是，我们加入了一个参数Hero（Context），这样做的目的是为了具体的状态类当达到某一个条件的时候可以切换上下文的状态。下面列出四个具体的状态类，其实就是把if else拆掉放到这几个类的run方法中。 1234567891011121314151617181920212223242526272829303132333435package com.state;public class CommonState implements RunState&#123; public void run(Hero hero) &#123; //正常跑动则不打印内容，否则会刷屏 &#125;&#125;public class SpeedUpState implements RunState&#123; public void run(Hero hero) &#123; System.out.println(&quot;--------------加速跑动---------------&quot;); try &#123; Thread.sleep(4000);//假设加速持续4秒 &#125; catch (InterruptedException e) &#123;&#125; hero.setState(Hero.COMMON); System.out.println(&quot;------加速状态结束，变为正常状态------&quot;); &#125; &#125;public class SwimState implements RunState&#123; public void run(Hero hero) &#123; System.out.println(&quot;--------------不能跑动---------------&quot;); try &#123; Thread.sleep(2000);//假设眩晕持续2秒 &#125; catch (InterruptedException e) &#123;&#125; hero.setState(Hero.COMMON); System.out.println(&quot;------眩晕状态结束，变为正常状态------&quot;); &#125;&#125; 这下我们的英雄类也要相应的改动一下，最主要的改动就是那些if else可以删掉了，如下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.state;//英雄类public class Hero &#123; public static final RunState COMMON = new CommonState();//正常状态 public static final RunState SPEED_UP = new SpeedUpState();//加速状态 public static final RunState SPEED_DOWN = new SpeedDownState();//减速状态 public static final RunState SWIM = new SwimState();//眩晕状态 private RunState state = COMMON;//默认是正常状态 private Thread runThread;//跑动线程 //设置状态 public void setState(RunState state) &#123; this.state = state; &#125; //停止跑动 public void stopRun() &#123; if (isRunning()) runThread.interrupt(); System.out.println(&quot;--------------停止跑动---------------&quot;); &#125; //开始跑动 public void startRun() &#123; if (isRunning()) &#123; return; &#125; final Hero hero = this; runThread = new Thread(new Runnable() &#123; public void run() &#123; while (!runThread.isInterrupted()) &#123; state.run(hero); &#125; &#125; &#125;); System.out.println(&quot;--------------开始跑动---------------&quot;); runThread.start(); &#125; private boolean isRunning()&#123; return runThread != null &amp;&amp; !runThread.isInterrupted(); &#125;&#125; 可以看到，现在我们的英雄类优雅了许多，我们使用刚才同样的客户端运行即可得到同样的结果。 对比我们的原始例子，现在我们使用状态模式之后，有几个明显的优点： 一、我们去掉了if else结构，使得代码的可维护性更强，不易出错，这个优点挺明显，如果试图让你更改跑动的方法，是刚才的一堆if else好改，还是分成了若干个具体的状态类好改呢？答案是显而易见的。 二、使用多态代替了条件判断，这样我们代码的扩展性更强，比如要增加一些状态，假设有加速20%，加速10%，减速10%等等等（这并不是虚构，DOTA当中是真实存在这些状态的），会非常的容易。 三、状态是可以被共享的，这个在上面的例子当中有体现，看下Hero类当中的四个static final变量就知道了，因为状态类一般是没有自己的内部状态的，所有它只是一个具有行为的对象，因此是可以被共享的。四、状态的转换更加简单安全，简单体现在状态的分割，因为我们把一堆if else分割成了若干个代码段分别放在几个具体的状态类当中，所以转换起来当然更简单，而且每次转换的时候我们只需要关注一个固定的状态到其他状态的转换。安全体现在类型安全，我们设置上下文的状态时，必须是状态接口的实现类，而不是原本的一个整数，这可以杜绝魔数以及不正确的状态码。 状态模式适用于某一个对象的行为取决于该对象的状态，并且该对象的状态会在运行时转换，又或者有很多的if else判断，而这些判断只是因为状态不同而不断的切换行为。 可以看到，这个类图与状态模式的标准类图是几乎一模一样的，只是多了一条状态接口到上下文的依赖线，而这个是根据实际需要添加的，而且一般情况下都是需要的。 状态模式也有它的缺点，不过它的缺点和大多数模式相似，有两点。 会增加的类的数量。 使系统的复杂性增加。尽管状态模式有着这样的缺点，但是往往我们牺牲复杂性去换取的高可维护性和扩展性是相当值得的，除非增加了复杂性以后，对于后者的提升会乎其微。 状态模式在项目当中也算是较经常会碰到的一个设计模式，但是通常情况下，我们还是在看到if else的情况下，对项目进行重构时使用，又或者你十分确定要做的项目会朝着状态模式发展，一般情况下，还是不建议在项目的初期使用。 好了，本次状态模式的分享就到此结束了，希望各位有所收获。 原地址https://www.cnblogs.com/zuoxiaolong/p/pattern26.html","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://www.iwhero.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://www.iwhero.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式的思考","slug":"设计模式/设计模式的思考","date":"2021-08-22T16:00:00.000Z","updated":"2021-11-16T07:05:56.788Z","comments":true,"path":"2021/08/23/设计模式/设计模式的思考/","link":"","permalink":"https://www.iwhero.com/2021/08/23/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%80%9D%E8%80%83/","excerpt":"","text":"","categories":[{"name":"设计模式","slug":"设计模式","permalink":"https://www.iwhero.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://www.iwhero.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"Kubernetes配置管理","slug":"K8S/12-Kubernetes配置管理","date":"2021-05-28T16:00:00.000Z","updated":"2021-11-26T07:48:35.480Z","comments":true,"path":"2021/05/29/K8S/12-Kubernetes配置管理/","link":"","permalink":"https://www.iwhero.com/2021/05/29/K8S/12-Kubernetes%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/","excerpt":"","text":"Kubernetes配置管理SecretSecret的主要作用就是加密数据，然后存在etcd里面，让Pod容器以挂载Volume方式进行访问 场景：用户名 和 密码进行加密 一般场景的是对某个字符串进行base64编码 进行加密 1echo -n &#x27;admin&#x27; | base64 变量形式挂载到Pod 创建secret加密数据的yaml文件 secret.yaml 然后使用下面命令创建一个pod 1kubectl create -f secret.yaml 通过get命令查看 1kubectl get pods 然后我们通过下面的命令，进入到我们的容器内部 1kubectl exec -it mypod bash 然后我们就可以输出我们的值，这就是以变量的形式挂载到我们的容器中 1234## 输出用户echo $SECRET_USERNAME## 输出密码echo $SECRET_PASSWORD 最后如果我们要删除这个Pod，就可以使用这个命令 1kubectl delete -f secret-val.yaml 数据卷形式挂载首先我们创建一个 secret-val.yaml 文件 然后创建我们的 Pod 123456## 根据配置创建容器kubectl apply -f secret-val.yaml## 进入容器kubectl exec -it mypod bash## 查看ls /etc/foo ConfigMapConfigMap作用是存储不加密的数据到etcd中，让Pod以变量或数据卷Volume挂载到容器中 应用场景：配置文件 创建配置文件首先我们需要创建一个配置文件 redis.properties 123redis.port=127.0.0.1redis.port=6379redis.password=123456 创建ConfigMap我们使用命令创建configmap 1kubectl create configmap redis-config --from-file=redis.properties 然后查看详细信息 1kubectl describe cm redis-config Volume数据卷形式挂载首先我们需要创建一个 cm.yaml 然后使用该yaml创建我们的pod 1234## 创建kubectl apply -f cm.yaml## 查看kubectl get pods 最后我们通过命令就可以查看结果输出了 1kubectl logs mypod 以变量的形式挂载Pod首先我们也有一个 myconfig.yaml文件，声明变量信息，然后以configmap创建 然后我们就可以创建我们的配置文件 1234## 创建podkubectl apply -f myconfig.yaml## 获取kubectl get cm 然后我们创建完该pod后，我们就需要在创建一个 config-var.yaml 来使用我们的配置信息 最后我们查看输出 1kubectl logs mypod","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes控制器Controller详解","slug":"K8S/11-Kubernetes控制器Controller详解","date":"2021-05-14T16:00:00.000Z","updated":"2021-11-26T07:48:35.501Z","comments":true,"path":"2021/05/15/K8S/11-Kubernetes控制器Controller详解/","link":"","permalink":"https://www.iwhero.com/2021/05/15/K8S/11-Kubernetes%E6%8E%A7%E5%88%B6%E5%99%A8Controller%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Kubernetes控制器Controller详解StatefulsetStatefulset主要是用来部署有状态应用 对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。 无状态应用我们原来使用 deployment，部署的都是无状态的应用，那什么是无状态应用？ 认为Pod都是一样的 没有顺序要求 不考虑应用在哪个node上运行 能够进行随意伸缩和扩展 有状态应用上述的因素都需要考虑到 让每个Pod独立的 让每个Pod独立的，保持Pod启动顺序和唯一性 唯一的网络标识符，持久存储 有序，比如mysql中的主从 适合StatefulSet的业务包括数据库服务MySQL 和 PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务 StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。 使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。 部署有状态应用无头service， ClusterIp：none 这里就需要使用 StatefulSet部署有状态应用 然后通过查看pod，能否发现每个pod都有唯一的名称 然后我们在查看service，发现是无头的service 这里有状态的约定，肯定不是简简单单通过名称来进行约定，而是更加复杂的操作 deployment：是有身份的，有唯一标识 statefulset：根据主机名 + 按照一定规则生成域名 每个pod有唯一的主机名，并且有唯一的域名 格式：主机名称.service名称.名称空间.svc.cluster.local 举例：nginx-statefulset-0.default.svc.cluster.local DaemonSetDaemonSet 即后台支撑型服务，主要是用来部署守护进程 长期伺服型和批处理型的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类的Pod运行；而后台支撑型服务的核心关注点在K8S集群中的节点(物理机或虚拟机)，要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点，也可能是通过 nodeSelector选定的一些特定节点。典型的后台支撑型服务包括：存储、日志和监控等。在每个节点上支撑K8S集群运行的服务。 守护进程在我们每个节点上，运行的是同一个pod，新加入的节点也同样运行在同一个pod里面 例子：在每个node节点安装数据采集工具 这里是不是一个FileBeat镜像，主要是为了做日志采集工作 进入某个 Pod里面，进入 1kubectl exec -it ds-test-cbk6v bash 通过该命令后，我们就能看到我们内部收集的日志信息了 Job和CronJob一次性任务 和 定时任务 一次性任务：一次性执行完就结束 定时任务：周期性执行 Job是K8S中用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别就是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的 spec.completions 策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功行任务保证有N个任务全部成功；工作队列性任务根据应用确定的全局成功而标志成功。 JobJob也即一次性任务 使用下面命令，能够看到目前已经存在的Job 1kubectl get jobs 在计算完成后，通过命令查看，能够发现该任务已经完成 我们可以通过查看日志，查看到一次性任务的结果 1kubectl logs pi-qpqff CronJob定时任务，cronjob.yaml如下所示 这里面的命令就是每个一段时间，这里是通过 cron 表达式配置的，通过 schedule字段 然后下面命令就是每个一段时间输出 我们首先用上述的配置文件，创建一个定时任务 1kubectl apply -f cronjob.yaml 创建完成后，我们就可以通过下面命令查看定时任务 1kubectl get cronjobs 我们可以通过日志进行查看 1kubectl logs hello-1599100140-wkn79 然后每次执行，就会多出一个 pod 删除svc 和 statefulset使用下面命令，可以删除我们添加的svc 和 statefulset 123kubectl delete svc webkubectl delete statefulset --all Replication ControllerReplication Controller 简称 RC，是K8S中的复制控制器。RC是K8S集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。 即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有一个Pod在运行。RC是K8S中较早期的技术概念，只适用于长期伺服型的业务类型，比如控制Pod提供高可用的Web服务。 Replica SetReplica Set 检查 RS，也就是副本集。RS是新一代的RC，提供同样高可用能力，区别主要在于RS后来居上，能够支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数来使用","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes核心技术Service","slug":"K8S/10-Kubernetes核心技术Service","date":"2021-05-09T16:00:00.000Z","updated":"2021-11-26T07:48:35.541Z","comments":true,"path":"2021/05/10/K8S/10-Kubernetes核心技术Service/","link":"","permalink":"https://www.iwhero.com/2021/05/10/K8S/10-Kubernetes%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFService/","excerpt":"","text":"Kubernetes核心技术Service前言前面我们了解到 Deployment 只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。 要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的后端服务实例。在K8S集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。 在K8S集群中，微服务的负载均衡是由kube-proxy实现的。kube-proxy是k8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8S的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。 Service存在的意义防止Pod失联【服务发现】因为Pod每次创建都对应一个IP地址，而这个IP地址是短暂的，每次随着Pod的更新都会变化，假设当我们的前端页面有多个Pod时候，同时后端也多个Pod，这个时候，他们之间的相互访问，就需要通过注册中心，拿到Pod的IP地址，然后去访问对应的Pod 定义Pod访问策略【负载均衡】页面前端的Pod访问到后端的Pod，中间会通过Service一层，而Service在这里还能做负载均衡，负载均衡的策略有很多种实现策略，例如： 随机 轮询 响应比 Pod和Service的关系这里Pod 和 Service 之间还是根据 label 和 selector 建立关联的 【和Controller一样】 我们在访问service的时候，其实也是需要有一个ip地址，这个ip肯定不是pod的ip地址，而是 虚拟IP vip Service常用类型Service常用类型有三种 ClusterIp：集群内部访问 NodePort：对外访问应用使用 LoadBalancer：对外访问应用使用，公有云 举例我们可以导出一个文件 包含service的配置信息 1kubectl expose deployment web --port=80 --target-port=80 --dry-run -o yaml &gt; service.yaml service.yaml 如下所示 12345678910111213141516apiVersion: v1kind: Servicemetadata: creationTimestamp: null labels: app: web name: webspec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: webstatus: loadBalancer: &#123;&#125; 如果我们没有做设置的话，默认使用的是第一种方式 ClusterIp，也就是只能在集群内部使用，我们可以添加一个type字段，用来设置我们的service类型 1234567891011121314151617apiVersion: v1kind: Servicemetadata: creationTimestamp: null labels: app: web name: webspec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: web type: NodePortstatus: loadBalancer: &#123;&#125; 修改完命令后，我们使用创建一个pod 1kubectl apply -f service.yaml 然后能够看到，已经成功修改为 NodePort类型了，最后剩下的一种方式就是LoadBalanced：对外访问应用使用公有云 node一般是在内网进行部署，而外网一般是不能访问到的，那么如何访问的呢？ 找到一台可以通过外网访问机器，安装nginx，反向代理 手动把可以访问的节点添加到nginx中 如果我们使用LoadBalancer，就会有负载均衡的控制器，类似于nginx的功能，就不需要自己添加到nginx上","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"使用ElasticSearch，Kibana，ASP.NET Core和Docker可视化数据","slug":"杂记/使用ElasticSearch，Kibana，ASP-NET-Core和Docker可视化数据","date":"2021-05-08T16:00:00.000Z","updated":"2021-11-16T07:06:30.927Z","comments":true,"path":"2021/05/09/杂记/使用ElasticSearch，Kibana，ASP-NET-Core和Docker可视化数据/","link":"","permalink":"https://www.iwhero.com/2021/05/09/%E6%9D%82%E8%AE%B0/%E4%BD%BF%E7%94%A8ElasticSearch%EF%BC%8CKibana%EF%BC%8CASP-NET-Core%E5%92%8CDocker%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE/","excerpt":"","text":"背景：目前在做酒店项目，酒店分多个板块，其中一块需要整合供应商酒店资源。考虑到酒店资源的数据会随这接入供应商的数量增多而增多，对数据存储会有很高的要求，综合考虑下还是选择了Elasticsearch。 原文地址：http://www.dotnetcurry.com/aspnet/1354/elastic-search-kibana-in-docker-dotnet-core-app 想要轻松地通过许多不同的方式查询数据，甚至是从未预料到的方式？想要以多种方式可视化日志？同时支持基于时间、文本和其他类型的即时过滤器？借助于 Elastic stack 的卓越性能和可扩展方式的优点，我们将通过两个示例轻松实现。 本文由 DNC Magazine for Developers and Architects 发布。 从这里下载此杂志[PDF] 或 免费订阅本杂志 下载所有以前和当前的版本版本。 在这篇文章中，我将介绍流行的搜索引擎 Elasticsearch，其配套的可视化应用 Kibana，并展示如何对.NET核心可以轻松地与 Elastic stack 整合在一块。 在这篇文章中，我将介绍流行的搜索引擎 Elasticsearch，其配套的可视化应用 Kibana，并展示如何对.NET核心可以轻松地与 Elastic stack 整合在一块。 Elasticsearch和.Net Core 我们将开始探索 Elasticsearch 的 REST API ，通过索引和查询某些数据。接着，我们将使用Elasticsearch官方的 .Net API 完成类似的练习。一旦熟悉 Elasticsearch 及其 API 后，我们将使用 .Net Core 创建一个日志模块，并将数据发送到 Elasticsearch 。Kibana紧随其中，以有趣的方式可视化 Elasticsearch 的索引数据。我迫切希望你会认为这篇文章十分有趣，并且想要了解更多关于Elastic的强大之处。 本文假设您已经了解 C＃和 REST API 的基本知识。使用 Visual Studio，Postman 和 Docker 等工具，但您可以轻松使用 VS Code 和 Fiddler 等替代方案。 Elasticsearch - 简介Elasticsearch 作为核心的部分，是一个具有强大索引功能的文档存储库，并且可以通过 REST API 来搜索数据。它使用 Java 编写，基于 Apache Lucene，尽管这些细节隐藏在 API 中。通过被索引的字段，可以用许多不同的聚合方式找到任何被存储(索引)的文档。但是，ElasticSearch不仅仅只提供对这些被索引文档的强大搜索功能。快速、分布式、水平扩展，支持实时文档存储和分析，支持数百台服务器和 PB 级索引数据。同时作为 Elastic stack (aka ELK) 的核心，提供了诸如 LogStash、Kibana 和更多的强大应用。Kibana 是 Elasticsearch 中专门提供强有力的可视化查询Web应用程序。使用Kibana，能非常简单地为 Elasticsearch 中索引的数据创建查询、图表和仪表盘。Elasticsearch开放了一个 REST API，你会发现许多文档示例是 HTTP 调用，你可以尝试使用 curl 或 postman 等工具。当然，这个 API 的客户端已经用许多不同的语言编写，包括.Net、Java、Python、Ruby和JavaScript等。如果你想阅读更多，Elasticsearch 官方网站 可能是最好的地方。 Docker是在本地运行的最简方式在这篇文章中，我们需要先连接到一个 Elasticsearch （和后面的Kibana）的服务器。如果您已经有一个在本地运行或可以使用的服务器，那很好。否则需要先搭建一个服务器。您可以选择在您的本地机器或可以使用的 VM 或服务器中下载和安装 Elasticsearch 和 Kibana 。不过，建议您使用最简单最纯粹的方式，使用Docker 搭建 Elasticsearch 和 Kibana 。您可以直接运行以下命令，获取包含Elasticsearch和Kibana的容器。 1docker run -it --rm -p 9200:9200 -p 5601:5601 --name esk nshou&#x2F; elasticsearch-kibana -it 表示以交互模式启动容器，并附加到终端。 –rm 表示从终端退出后，容器将被移除。 -p 将容器中的端口映射到主机中的端口 –name 给容器一个名称，当您不使用的情况下可以用 –rm 手动停止/删除 nshou/elasticsearch-kibana 是 Docker Hub中的一个镜像的名称，已经有人帮你准备好了Elasticsearch和Kibana 如果你喜欢在后台运行的话，你可以使用参数-d 代替 –it –rm，并且手动停止/删除容器。 在同一个容器中运行多个应用程序，就像我们现在这种做法，非常适用本文，但不是推荐用于生产容器！ 您应该意识到，一旦你删除容器，你的数据就会消失（一旦你使用-rm选项就删除它了）。虽然有利于本地实验，但在实际环境中，如果您不想丢失数据，请参照 “data container” 模式。 Docker是一个很棒的工具，我鼓励你更多地了解它，特别是如果你想做更重要的事情，而不仅仅是跟随本文，在本地快速搭建 Elasticsearch 服务器。在之前的文章 Building DockNetFiddle using Docker and .NET Core 中已经对 .NET Core 搭配 Docker 有很好的介绍。 只需打开 http://localhost:9200 和 http://localhost:5600 ，检查Elasticsearch 和 Kibana 是否都可以使用。（如果您使用docker toolbox，请使用托管Docker的虚拟机ip替换localhost，您可以在命令行中运行 docker-machine env default ）。 在docker中运行 Elasticsearch kibana也准备好了 在 Elasticsearch 中索引和查询在我们开始编写任何 .Net 代码之前，我们先了解一下一些基本知识。先在 Elasticsearch 索引一些文档（类似于存到数据库），以便我们对它们运行不同的查询。 在这里，我将使用Postman向我们的 Elasticsearch 服务器发送 HTTP 请求，但您可以使用任何其他类似的工具，如 Fiddler或 curl 。 我们要做的第一件事是请求 Elasticsearch 创建一个新的索引 (译者语:类似创建一个表) 并索引一些文档 (译者语:类似于在数据中插入数据) 。这类似于将数据存储在表/集合中，主要区别（和目的）是让 Elasticsearch 集群 （这里只是一个节点） 可以分析和搜索文档数据。被索引的文档在 Elasticsearch 中以索引和类型进行组织。以往，被拿来和数据库表做对比，往往会令人困惑。如这篇文章所述，索引由Lucene处理，在分布式跨 分片 中，与类型紧密地联系在一起。发送以下两个请求以创建索引，并在该索引中插入文档 (请记住 toolbox，如果使用docker ，请使用托管Docker的虚拟机ip而不是localhost) : 创建一个名为 “default” 的新索引 1PUT localhost:9200&#x2F;default 在 “default” 索引中索引文档。请注意，我们需要知道我们存储哪种类型的文档（”product”）和该文档的ID (如 1，尽管您可以使用任何值，只要它是唯一的)123456PUT localhost:9200&#x2F;default&#x2F;product&#x2F;1&#123; &quot;name&quot;: &quot;Apple MacBook Pro&quot;, &quot;description&quot;: &quot;Latest MacBook Pro 13&quot;, &quot;tags&quot;: [&quot;laptops&quot;, &quot;mac&quot;]&#125; 创建一个新索引 索引新文档 在我们验证搜索功能和查询数据之前，再索引几个 “product”。尝试使用不同的 “tags”，如 “laptops”和 “laptops”，并记得使用不同的ids！完成后，让我们按名称排序的搜索所有被索引的文档。您可以使用查询字符串或 GET/POST 同样的内容，下面两个请求是等效的： 12345678GET http:&#x2F;&#x2F;localhost:9200&#x2F;default&#x2F;_search?q&#x3D;*&amp;sort&#x3D;name.keyword:ascPOST http:&#x2F;&#x2F;localhost:9200&#x2F;default&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;name.keyword&quot;: &quot;asc&quot; &#125; ]&#125; 让我们尝试一些更有趣的东西，例如搜索 “description” 字段中含有 “latest” ，同时 “tags” 字段中含有 “laptops” 的所有文档： 1234567891011121314POST http:&#x2F;&#x2F;localhost:9200&#x2F;default&#x2F;_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123;&quot;description&quot;: &quot;latest&quot;&#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;tags&quot;: &quot;laptops&quot; &#125; &#125; ] &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;name.keyword&quot;: &quot;asc&quot; &#125; ]&#125; 搜索结果 Kibana 可视化数据作为介绍的最后部分，我们将对 Kibana 的相关知识蜻蜓点水。假设您在上一步已经索引了几个文档，通过访问 http://localhost:5601 中打开在 Docker 的 Kibana 服务器。你会注意到，Kibana 要求你提供默认的索引模式，所以必须告诉它使用的 Elasticsearch 索引： 我们在上一节中创建了一个名为 “default” 的索引，因此可以使用 “default” 作为索引模式。您还需要取消 “索引包含基于时间的事件 (Index contains time-based events ) “ 选项，因为我们的文档不包含任何时间字段。 在 Kibana 中添加索引模式 完成后，使用左侧菜单打开 “ 发现 (Discover) “ 页面，您应该会看到上一节中插入的所有最新文档。尝试选择不同的字段，在搜索栏中输入相关的字段或某个过滤器： 在 kibana 中可视化数据 最后，我们创建一个饼图，显示 “laptops” 或 “desktops” 的销量百分比。利用之前索引的数据，在左侧菜单新建一个 “饼图 (Pie Chart)” 。您可以在 饼图 (Pie Chart)的页面上配置。将 “ Count “ 作为切片的大小，并在 “ buckets “ 部分中选择 “ split slices “ 。将 “ filters “ 作为聚合类型，添加两个过滤器：tags =”laptop” 和 tags =”desktoptops” 。单击运行，您将看到类似于下图： 在Kibana中创建饼图 确保在搜索栏中输入包含已过滤的项目的搜索关键词，并注意到可视化图形如何变化。 Elasticsearch .Net API在简要介绍Elasticsearch和Kibana之后，我们来看看我们如何用 .Net 应用程序索引和查询我们的文档。您可能想知道为什么要这样做，而不是直接使用 HTTP API 。我可以提供几个理由，我相信你可以自己找几个： 你不想直接暴露 Elasticsearch 集群 Elasticsearch 可能不是您的主数据库，您可能需要结合来自主数据库的结果。 你希望包含来自存储/生产服务器中的被索引文档首先需要注意的是打开 这个文档 ,有两个官方提供的 APIs : Elasticsearch.Net 和 NEST ，都支持 .Net Core 项目。 Elasticsearch.Net 提供了一个用于与 Elasticsearch连接的低级API，提供构建/处理请求和响应的功能。它是 .Net 瘦客户端。 NEST 在 Elasticsearch.Net 之上，提供了更高级别的 API 。它可以将对象映射到请求/响应中，提供强大查询功能，将索引名称、文档类型、字段类型用于构建与 HTTP REST API 的匹配查询。 Elasticsearch .Net API 由于我使用的是 NEST，所以第一步是创建一个新的 ASP .Net Core 应用程序，并使用 Package Manager 安装NEST。 使用Nest开始索引数据我们将在新的 ASP.Net Core 应用程序中完成之前手动发送 HTTP 请求的一些步骤。如果需要，请重新启Docker 容器，从而清理数据；或通过 HTTP API 和 Postman 手动删除文档/索引。我们首先为产品创建一个POCO模型： 1234567public class Product&#123; public Guid Id &#123; get; set; &#125; public string Name &#123; get; set; &#125; public string Description &#123; get; set; &#125; public string[] Tags &#123; get; set; &#125; &#125; 接下来，我们创建一个新的控制器 ProductController，它具有添加新的 “Product” 的方法和基于单个关键词查找 “Product” 的方法： 12345678910111213public class ProductController : Controller &#123; [HttpPost] public async Task&lt; IActionResult &gt; Create([FromBody]Product product) &#123; &#125; [HttpGet(&quot;find&quot;)] public async Task&lt; IActionResult &gt; Find(string term) &#123; &#125; &#125; 为了实现这些方法，我们需要先连接到 Elasticsearch。这里有一个 ElasticClient 连接的正确示范。 由于该类是线程安全的，所以推荐的方法是在应用程序中使用单例模式，而不是按请求创建新的连接。 为了简洁起见，我现在将使用带有硬编码设置的私有静态变量。在 .Net Core 中使用依赖注入配置框架，或查看 Github中 的代码。 可以想到的是，至少需要提供被连接的 Elasticsearch 集群的URL。当然，还有其他可选参数，用于与您的群集进行身份验证、设置超时、连接池等。 1234 private static readonly ConnectionSettings connSettings &#x3D; new ConnectionSettings(new Uri(&quot;http:&#x2F;&#x2F;localhost:9200&#x2F;&quot;)); private static readonly ElasticClient elasticClient &#x3D; new ElasticClient(connSettings); 建立连接后，索引文档只是简单地使用 ElasticClient 的Index/IndexAsync 方法： 1234567891011121314[Route(&quot;api&#x2F;[controller]&quot;)]public class ProductController : Controller&#123; [HttpPost] public async Task&lt;IActionResult&gt; Create([FromBody]Product product) &#123; &#125; [HttpGet(&quot;find&quot;)] public async Task&lt;IActionResult&gt; Find(string term) &#123; &#125;&#125; 很简单，对吧？不幸的是，如果您向Postman发送以下请求，您将看到失败。 123456POST http:&#x2F;&#x2F;localhost:65113&#x2F;api&#x2F;product&#123; &quot;name&quot;: &quot;Dell XPS 13&quot;, &quot;description&quot;: &quot;Latest Dell XPS 13&quot;, &quot;tags&quot;: [&quot;laptops&quot;, &quot;windows&quot;]&#125; 这是因为NEST无法确定在索引文档时使用哪个索引！如果您想起手动使用 HTTP API 的做法，那么需要在URL指出文档的索引、文档的类型和ID，如 localhost:9200/default/product/1 NEST能够推断文档的类型（使用类的名称），还可以默认对字段进行索引（基于字段的类型），但需要一些索引名称的帮助。您可以指定默认的索引名称，以及特定类型的特定索引名称。 12345connSettings &#x3D; new ConnectionSettings(new Uri(&quot;http:&#x2F;&#x2F;192.168.99.100:9200&#x2F;&quot;)) .DefaultIndex(&quot;default&quot;) &#x2F;&#x2F;Optionally override the default index for specific types .MapDefaultTypeIndices(m &#x3D;&gt; m .Add(typeof(Product), &quot;default&quot;)); 进行这些更改后再试一次。您将看到 NEST 创建索引(如果尚未存在)，并将文档编入索引。如果你切换到 Kibana，你也可以看到该文档。需要注意的是: 从类的名称推断文档类型，如 Product在类中将Id属性推断为标识将所有公开的属性发送到 Elasticsearch 使用NEST索引的文档 在我们查询数据之前，重新考虑创建索引的方式。如何创建索引?现在我们得到一个事实，即如果这个索引不存在，也会被创建。然而映射字段的索引方式很重要，并直接定义了 Elasticsearch 如何索引和分析这些字段。这对于字符串字段尤其明显，因为在 Elasticsearch v5 中提供了两种不同字段类型的 “Text” 和 “Keyword”： Text 类型的字段将会被分析和分解成单词，以便用于更高级的 Elasticsearch 搜索功能 另一方面，Keyword 字段将 “保持原样” 而不进行分析，只能通过其精确值进行搜索。您可以使用 NEST 索引映射属性来生成POCO模型： 12345678910public class Product&#123; public Guid Id &#123; get; set; &#125; [Text(Name&#x3D;&quot;name&quot;)] public string Name &#123; get; set; &#125; [Text(Name &#x3D; &quot;description&quot;)] public string Description &#123; get; set; &#125; [Keyword(Name &#x3D; &quot;tag&quot;)] public string[] Tags &#123; get; set; &#125; &#125; 然而，我们需要先创建索引，必须使用 ElasticClient API 手动创建和定义索引的映射。这是非常简单的，特别是如果我们只是使用属性： 123456if (!elasticClient.IndexExists(&quot;default&quot;).Exists) &#123; elasticClient.CreateIndex(&quot;default&quot;, i &#x3D;&gt; i .Mappings(m &#x3D;&gt; m .Map&lt;Product&gt;(ms &#x3D;&gt; ms.AutoMap()))); &#125; 直接向Elasticsearch发送请求(GET localhost:92000/default)，并注意与我们想要的映射是否相同。 使用NEST创建索引映射 使用Nest查询数据现在，我们有一个使用 NEST 对 “products” 进行索引的 ProductController 控制器。是时候，为这个控制器添加 Find action，用于使用 NEST 向 Elasticsearch 查询文档。我们只是用到一个字段来实现一个简单的搜索。您应该观察所有字段: 映射为 “Text” 类型的字段可以被分析，您可以在 “name” / “description” 字段内搜索特定的单词 映射为 “Keywords” 的字段是保持原样的，未进行分析。您只能在 “tags” 字段中完全匹配。 NEST 提供了一个查询 Elasticsearch 的丰富 API，可以转换成标准的 HTTP API 。实现上述查询类型与使用Search/SearchAsync方法一样简单，并构建一个 SimpleQueryString 作为参数。 12345678910111213[HttpGet(&quot;find&quot;)]public async Task&lt;IActionResult&gt; Find(string term)&#123; var res &#x3D; await elasticClient.SearchAsync&lt;Product&gt;(x &#x3D;&gt; x .Query( q &#x3D;&gt; q. SimpleQueryString(qs &#x3D;&gt; qs.Query(term)))); if (!res.IsValid) &#123; throw new InvalidOperationException(res.DebugInformation); &#125; return Json(res.Documents);&#125; 使用PostMan测试您的新操作： 使用nest查询 正如您可能已经意识到的那样，我们的操作行为与手动发送请求到 Elasticsearch 一样： GET http://localhost:9200/default/_search?q=*&amp; 在 .Net Core 中创建一个Elasticsearch日志提供程序现在我们了解了 NEST 的一些基础知识，让我们尝试一些更有野心的事情。我们已经创建了一个 ASP.Net Core 的应用程序，借助.NET Core的日志框架，实现我们的日志提供程序，并将信息发送到Elasticsearch。新的日志 API 在日志 (logger) 和日志提供程序 (logger provider) 方面的区别： 日志 (logger) 记录信息和事件，如用于控制器中 可以为应用程序添加并启用多个日志提供程序 (provider) ，并可以配置独立的记录级别和记录相应的信息/事件。 该日志框架内置了一些对事件日志、Azure 等的日志提供程序 (provider)，但正如您将看到的，创建自己的并不复杂。有关详细信息，请查阅.NET Core 关于日志的官方文档。 在本文的最后部分，我们将为Elasticsearch创建一个新的日志提供程序，在我们的应用程序中启用它，并使用Kibana来查看记录的事件。 为Elasticsearch添加一个新的日志提供程序首先要做的是定义一个新的POCO对象，我们将使用它作为使用NEST进行索引的文档，类似于之前创建的 “Product” 类。这将包含有关可能发生的任何异常以及相关请求数据的记录信息、可选信息。记录请求数据将会派上用场，因为我们可以根据具体请求查询/可视化我们记录的事件。 12345678910111213141516171819202122232425262728293031323334353637383940public class LogEntry&#123; public DateTime DateTime &#123; get; set; &#125; public EventId EventId &#123; get; set; &#125; [Keyword] [JsonConverter(typeof(StringEnumConverter))] public Microsoft.Extensions.Logging.LogLevel Level &#123; get; set; &#125; [Keyword] public string Category &#123; get; set; &#125; public string Message &#123; get; set; &#125; [Keyword] public string TraceIdentifier &#123; get; set; &#125; [Keyword] public string UserName &#123; get; set; &#125; [Keyword] public string ContentType &#123; get; set; &#125; [Keyword] public string Host &#123; get; set; &#125; [Keyword] public string Method &#123; get; set; &#125; [Keyword] public string Protocol &#123; get; set; &#125; [Keyword] public string Scheme &#123; get; set; &#125; public string Path &#123; get; set; &#125; public string PathBase &#123; get; set; &#125; public string QueryString &#123; get; set; &#125; public long? ContentLength &#123; get; set; &#125; public bool IsHttps &#123; get; set; &#125; public IRequestCookieCollection Cookies &#123; get; set; &#125; public IHeaderDictionary Headers &#123; get; set; &#125; [Keyword] public string ExceptionType &#123; get; set; &#125; public string ExceptionMessage &#123; get; set; &#125; public string Exception &#123; get; set; &#125; public bool HasException &#123; get &#123; return Exception !&#x3D; null; &#125; &#125; public string StackTrace &#123; get; set; &#125;&#125; 下一步是在一个新类上实现ILogger接口。如您所想，这将需要记录的数据映射到一个新的 LogEntry 对象，并使用 ElasticClient 对其进行索引。 我们将使用IHttpContextAccessor，以便我们可以获取当前的HttpContext并提取相关的请求属性。 在这里就不写连接到Elasticsearch并创建索引的代码，这与之前的操作，没有什么不同。使用不同的索引或删除上一节中索引的 “products” 。 注意： 您可以使用依赖注入和配置文检查 Github 中 的配套代码。 实现的主要方法是Log ，这是我们创建一个LogEntry并用NEST进行索引： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public void Log&lt; TState &gt;(LogLevel logLevel, EventId eventId, TState state, Exception exception, Func&lt; TState, Exception, string &gt; formatter)&#123; if (!IsEnabled(logLevel)) return; var message &#x3D; formatter(state, exception); var entry &#x3D; new LogEntry &#123; EventId &#x3D; eventId, DateTime &#x3D; DateTime.UtcNow, Category &#x3D; _categoryName, Message &#x3D; message, Level &#x3D; logLevel &#125;; var context &#x3D; _httpContextAccessor.HttpContext; if (context !&#x3D; null) &#123; entry.TraceIdentifier &#x3D; context.TraceIdentifier; entry.UserName &#x3D; context.User.Identity.Name; var request &#x3D; context.Request; entry.ContentLength &#x3D; request.ContentLength; entry.ContentType &#x3D; request.ContentType; entry.Host &#x3D; request.Host.Value; entry.IsHttps &#x3D; request.IsHttps; entry.Method &#x3D; request.Method; entry.Path &#x3D; request.Path; entry.PathBase &#x3D; request.PathBase; entry.Protocol &#x3D; request.Protocol; entry.QueryString &#x3D; request.QueryString.Value; entry.Scheme &#x3D; request.Scheme; entry.Cookies &#x3D; request.Cookies; entry.Headers &#x3D; request.Headers; &#125; if (exception !&#x3D; null) &#123; entry.Exception &#x3D; exception.ToString(); entry.ExceptionMessage &#x3D; exception.Message; entry.ExceptionType &#x3D; exception.GetType().Name; entry.StackTrace &#x3D; exception.StackTrace; &#125; elasticClient.Client.Index(entry);&#125; 您还需要额外实现 BeginScope 和 IsEnabled 方法。 为了本文的目的，忽略 BeginScope，只返回null。 更新您的构造函数，以便它接收一个日志级别(LogLevel)，如果接收到大于或等于构造函数中的日志级别，则实现 IsEnabled 并返回 true。 你可能会问为什么需要分类？这是一个用于标识日志是哪种类型的字符串。默认情况下，每次注入ILogger 的实例时，该类别默认分配为T的分类名称。例如，获取ILogger 并使用它来记录某些事件，意味着这些事件将具有 “MyController “ 名称。这可能派上用场，例如为不同的类设置不同的日志级别，以过滤/查询记录的事件。我相信您可能还想到更多的用法。 这个类的实现将如下所示： 1234567891011121314151617181920212223242526272829303132333435363738public class ESLoggerProvider: ILoggerProvider&#123; private readonly IHttpContextAccessor _httpContextAccessor; private readonly FilterLoggerSettings _filter; public ESLoggerProvider(IServiceProvider serviceProvider, FilterLoggerSettings filter &#x3D; null) &#123; _httpContextAccessor &#x3D; serviceProvider.GetService&lt;IHttpContextAccessor&gt;(); _filter &#x3D; filter ?? new FilterLoggerSettings &#123; &#123;&quot;*&quot;, LogLevel.Warning&#125; &#125;; &#125; public ILogger CreateLogger(string categoryName) &#123; return new ESLogger(_httpContextAccessor, categoryName, FindLevel(categoryName)); &#125; private LogLevel FindLevel(string categoryName) &#123; var def &#x3D; LogLevel.Warning; foreach (var s in _filter.Switches) &#123; if (categoryName.Contains(s.Key)) return s.Value; if (s.Key &#x3D;&#x3D; &quot;*&quot;) def &#x3D; s.Value; &#125; return def; &#125; public void Dispose() &#123; &#125;&#125; 最后，我们创建一个扩展方法，可以用于在启动类中注册我们的日志提供程序： 123456789101112131415161718public static class LoggerExtensions&#123; public static ILoggerFactory AddESLogger(this ILoggerFactory factory, IServiceProvider serviceProvider, FilterLoggerSettings filter &#x3D; null) &#123; factory.AddProvider(new ESLoggerProvider(serviceProvider, filter)); return factory; &#125;&#125;public void Configure(IApplicationBuilder app, IHostingEnvironment env, ILoggerFactory loggerFactory)&#123; loggerFactory.AddConsole(Configuration.GetSection(&quot;Logging&quot;)) .AddDebug() .AddESLogger(app.ApplicationServices, new FilterLoggerSettings &#123; &#123;&quot;*&quot;, LogLevel.Information&#125; &#125;); …&#125; 请注意我如何覆盖默认设置并按日志级别分类记录。这样，我们可以轻松地为每个请求索引一些事件。 在Kibana中可视化数据现在我们已经在Kibana中记录了事件，我们来探索数据可视化吧！首先，在Kibana中重建索引，这次确保选择” Index contains time-based events ( Index包含基于时间的事件 )”，选择字段dateTime作为”Time-field name (时间字段名称)”。接下来，启动您的应用程序，浏览一些页面以获取一些事件日志。还可以在某个端点随意添加抛出异常的代码，以便我们可以看到被记录的异常数据。在这之后，请转到Kibana的 发现 (Discover) 页面，您可以看到由 “dateTime” 字段排序的多个事件（默认情况下，数据被过滤为最近15分钟，但您可以在右上角更改）： Kibana可视化中记录的事件 试着在搜索栏中输入 “exception”，并注意任何一个被分析的文本字段中包含 “exception” 的事件。然后尝试搜索特定的异常类型（记住我们使用了一个关键字字段！）。您还可以尝试搜索特定的URL，如以 “ /Home/About “和” /Home/About” 路径的两种搜索方式 。您会注意到第一种情况包括引用者是 “/Home/About” 的事件，而第二种情况则只能正确返回路径为 “/Home/About” 的事件。一旦你熟悉了数据，以及如何查询数据，那么可以用数据创建一些有趣的图形。首先，我们将创建一个图表，显示每分钟记录的异常数。 转到Kibana的 可视化 (Visualize) 页面，并创建一个新的 垂直条形图 (Vertical bar chart) 。 选择Y轴作为计数，X轴上为日期的直方图。 将间隔设置为每分钟，最后在搜索框中添加一个过滤器 “hasException：true” 。一个很棒的图表，显示每分钟记录的异常数目： 每分钟记录的异常数目 接下来，显示每个 category 随时间记录的消息数量，限于前5个 category ： 转到Kibana的 可视化 (Visualize) 页面，并创建一个新的 线型图 (Line chart) 。 再次选择Y轴作为计数，X轴上为日期的直方图，选择dateTime作为字段，间隔为每分钟。 现在添加一个 sub-bucket 并选择 “split lines” 。使用 “significant terms” 作为聚合，category 为字段，单位为5个。这将绘制类似于以下的图表： 随着时间的推移 尝试在搜索框中添加一些过滤器，并查看它对结果的影响。最后，我们添加另一个图表，我们将看到前五个出现最多的消息和前五个 categories 的消息。 转到Kibana的 可视化 (Visualize) 页面，并创建一个新的 饼图 (Pie chart) 。 像之前一样，选择Y轴的计数 现在，将 “Terms” 作为聚合，将 “category” 作为字段，数量作为单位，限制前五个，画出图表。 然后将 “Terms” 作为聚合来分割切片，”message.keyword” 作为字段，数量作为单位，限制前五个。一旦你有了这些设置，你会看到一个类似于这个图表： 每个 category 中最常见的消息 花时间观察下数据(百分比，message/category 显示在图表元素上)。例如，您将观察到由DeveloperExceptionPageMiddleware类记录的异常。 结论Elasticsearch是一个强大的数据索引和查询平台。虽然它本身相当令人印象深刻，但与其他应用程序（如Kibana）相结合，可以很好地分析、报告和可视化数据。只要您开始使用，只是蜻蜓点水都能的到非凡的结果。对于 .Net 和 .Net Core，Elasticsearch 官方的 API 已经覆盖，因为它们支持 .Net Standard 1.3和更高版本（他们仍然在为1.1提供支持）。正如我们已经看到的，在 ASP.Net Core 项目中使用这个 API 是很方便的，我们可以轻松地将其 REST API 作为存储，以及在应用程序中作为日志提供程序。最后但并非不重要的一点，我希望您使用Docker。尝试使用 Elasticsearch，同时思考Docker可以为您和您的团队做些什么。 下载本文的全部源代码(Github)。","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://www.iwhero.com/categories/elasticsearch/"},{"name":"docker","slug":"docker","permalink":"https://www.iwhero.com/categories/docker/"},{"name":"kibana","slug":"kibana","permalink":"https://www.iwhero.com/categories/kibana/"},{"name":"net","slug":"net","permalink":"https://www.iwhero.com/categories/net/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://www.iwhero.com/tags/elasticsearch/"},{"name":"docker","slug":"docker","permalink":"https://www.iwhero.com/tags/docker/"},{"name":"Kibana","slug":"Kibana","permalink":"https://www.iwhero.com/tags/Kibana/"}]},{"title":"Kubernetes核心技术Controller","slug":"K8S/9-Kubernetes核心技术Controller","date":"2021-05-04T16:00:00.000Z","updated":"2021-11-26T07:48:35.221Z","comments":true,"path":"2021/05/05/K8S/9-Kubernetes核心技术Controller/","link":"","permalink":"https://www.iwhero.com/2021/05/05/K8S/9-Kubernetes%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFController/","excerpt":"","text":"Kubernetes核心技术-Controller内容 什么是Controller Pod和Controller的关系 Deployment控制器应用场景 yaml文件字段说明 Deployment控制器部署应用 升级回滚 弹性伸缩 什么是ControllerController是在集群上管理和运行容器的对象，Controller是实际存在的，Pod是虚拟机的 Pod和Controller的关系Pod是通过Controller实现应用的运维，比如弹性伸缩，滚动升级等 Pod 和 Controller之间是通过label标签来建立关系，同时Controller又被称为控制器工作负载 Deployment控制器应用 Deployment控制器可以部署无状态应用 管理Pod和ReplicaSet 部署，滚动升级等功能 应用场景：web服务，微服务 Deployment表示用户对K8S集群的一次更新操作。Deployment是一个比RS( Replica Set, RS) 应用模型更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧RS中的副本数减少到0的复合操作。 这样一个复合操作用一个RS是不好描述的，所以用一个更通用的Deployment来描述。以K8S的发展方向，未来对所有长期伺服型的业务的管理，都会通过Deployment来管理。 Deployment部署应用之前我们也使用Deployment部署过应用，如下代码所示 1kubectrl create deployment web --image=nginx 但是上述代码不是很好的进行复用，因为每次我们都需要重新输入代码，所以我们都是通过YAML进行配置 但是我们可以尝试使用上面的代码创建一个镜像【只是尝试，不会创建】 1kubectl create deployment web --image=nginx --dry-run -o yaml &gt; nginx.yaml 然后输出一个yaml配置文件 nginx.yml ，配置文件如下所示 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: web name: webspec: replicas: 1 selector: matchLabels: app: web strategy: &#123;&#125; template: metadata: creationTimestamp: null labels: app: web spec: containers: - image: nginx name: nginx resources: &#123;&#125;status: &#123;&#125; 我们看到的 selector 和 label 就是我们Pod 和 Controller之间建立关系的桥梁 使用YAML创建Pod通过刚刚的代码，我们已经生成了YAML文件，下面我们就可以使用该配置文件快速创建Pod镜像了 1kubectl apply -f nginx.yaml 但是因为这个方式创建的，我们只能在集群内部进行访问，所以我们还需要对外暴露端口 1kubectl expose deployment web --port=80 --type=NodePort --target-port=80 --name=web1 关于上述命令，有几个参数 –port：就是我们内部的端口号 –target-port：就是暴露外面访问的端口号 –name：名称 –type：类型 同理，我们一样可以导出对应的配置文件 1kubectl expose deployment web --port=80 --type=NodePort --target-port=80 --name=web1 -o yaml &gt; web1.yaml 得到的web1.yaml如下所示 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: v1kind: Servicemetadata: creationTimestamp: &quot;2020-11-16T02:26:53Z&quot; labels: app: web managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:labels: .: &#123;&#125; f:app: &#123;&#125; f:spec: f:externalTrafficPolicy: &#123;&#125; f:ports: .: &#123;&#125; k:&#123;&quot;port&quot;:80,&quot;protocol&quot;:&quot;TCP&quot;&#125;: .: &#123;&#125; f:port: &#123;&#125; f:protocol: &#123;&#125; f:targetPort: &#123;&#125; f:selector: .: &#123;&#125; f:app: &#123;&#125; f:sessionAffinity: &#123;&#125; f:type: &#123;&#125; manager: kubectl operation: Update time: &quot;2020-11-16T02:26:53Z&quot; name: web2 namespace: default resourceVersion: &quot;113693&quot; selfLink: /api/v1/namespaces/default/services/web2 uid: d570437d-a6b4-4456-8dfb-950f09534516spec: clusterIP: 10.104.174.145 externalTrafficPolicy: Cluster ports: - nodePort: 32639 port: 80 protocol: TCP targetPort: 80 selector: app: web sessionAffinity: None type: NodePortstatus: loadBalancer: &#123;&#125; 然后我们可以通过下面的命令来查看对外暴露的服务 1kubectl get pods,svc 然后我们访问对应的url，即可看到 nginx了 http://192.168.177.130:32639/ 升级回滚和弹性伸缩 升级： 假设从版本为1.14 升级到 1.15 ，这就叫应用的升级【升级可以保证服务不中断】 回滚：从版本1.15 变成 1.14，这就叫应用的回滚 弹性伸缩：我们根据不同的业务场景，来改变Pod的数量对外提供服务，这就是弹性伸缩 应用升级和回滚首先我们先创建一个 1.14版本的Pod 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: web name: webspec: replicas: 1 selector: matchLabels: app: web strategy: &#123;&#125; template: metadata: creationTimestamp: null labels: app: web spec: containers: - image: nginx:1.14 name: nginx resources: &#123;&#125;status: &#123;&#125; 我们先指定版本为1.14，然后开始创建我们的Pod 1kubectl apply -f nginx.yaml 同时，我们使用docker images命令，就能看到我们成功拉取到了一个 1.14版本的镜像 我们使用下面的命令，可以将nginx从 1.14 升级到 1.15 1kubectl set image deployment web nginx=nginx:1.15 在我们执行完命令后，能看到升级的过程 首先是开始的nginx 1.14版本的Pod在运行，然后 1.15版本的在创建 然后在1.15版本创建完成后，就会暂停1.14版本 最后把1.14版本的Pod移除，完成我们的升级 我们在下载 1.15版本，容器就处于ContainerCreating状态，然后下载完成后，就用 1.15版本去替换1.14版本了，这么做的好处就是：升级可以保证服务不中断 我们到我们的node2节点上，查看我们的 docker images; 能够看到，我们已经成功拉取到了 1.15版本的nginx了 查看升级状态下面可以，查看升级状态 1kubectl rollout status deployment web 查看历史版本我们还可以查看历史版本 1kubectl rollout history deployment web 应用回滚我们可以使用下面命令，完成回滚操作，也就是回滚到上一个版本 1kubectl rollout undo deployment web 然后我们就可以查看状态 1kubectl rollout status deployment web 同时我们还可以回滚到指定版本 1kubectl rollout undo deployment web --to-revision=2 弹性伸缩弹性伸缩，也就是我们通过命令一下创建多个副本 1kubectl scale deployment web --replicas=10 能够清晰看到，我们一下创建了10个副本","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes核心技术Pod","slug":"K8S/8-Kubernetes核心技术Pod","date":"2021-04-30T16:00:00.000Z","updated":"2021-11-26T07:48:35.243Z","comments":true,"path":"2021/05/01/K8S/8-Kubernetes核心技术Pod/","link":"","permalink":"https://www.iwhero.com/2021/05/01/K8S/8-Kubernetes%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFPod/","excerpt":"","text":"Kubernetes核心技术PodPod概述Pod是K8S系统中可以创建和管理的最小单元，是资源对象模型中由用户创建或部署的最小资源对象模型，也是在K8S上运行容器化应用的资源对象，其它的资源对象都是用来支撑或者扩展Pod对象功能的，比如控制器对象是用来管控Pod对象的，Service或者Ingress资源对象是用来暴露Pod引用对象的，PersistentVolume资源对象是用来为Pod提供存储等等，K8S不会直接处理容器，而是Pod，Pod是由一个或多个container组成。 Pod是Kubernetes的最重要概念，每一个Pod都有一个特殊的被称为 “根容器”的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。 Pod基本概念 最小部署的单元 Pod里面是由一个或多个容器组成【一组容器的集合】 一个pod中的容器是共享网络命名空间 Pod是短暂的 每个Pod包含一个或多个紧密相关的用户业务容器 Pod存在的意义 创建容器使用docker，一个docker对应一个容器，一个容器运行一个应用进程 Pod是多进程设计，运用多个应用程序，也就是一个Pod里面有多个容器，而一个容器里面运行一个应用程序 Pod的存在是为了亲密性应用 两个应用之间进行交互 网络之间的调用【通过127.0.0.1 或 socket】 两个应用之间需要频繁调用 Pod是在K8S集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。同时Pod对多容器的支持是K8S中最基础的设计理念。在生产环境中，通常是由不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。 Pod是K8S集群中所有业务类型的基础，可以把Pod看作运行在K8S集群上的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8S的业务主要可以分为以下几种 长期伺服型：long-running 批处理型：batch 节点后台支撑型：node-daemon 有状态应用型：stateful application 上述的几种类型，分别对应的小机器人控制器为：Deployment、Job、DaemonSet 和 StatefulSet (后面将介绍控制器) Pod实现机制主要有以下两大机制 共享网络 共享存储 共享网络容器本身之间相互隔离的，一般是通过 namespace 和 group 进行隔离，那么Pod里面的容器如何实现通信？ 首先需要满足前提条件，也就是容器都在同一个namespace之间 关于Pod实现原理，首先会在Pod会创建一个根容器： pause容器，然后我们在创建业务容器 【nginx，redis 等】，在我们创建业务容器的时候，会把它添加到 info容器 中 而在 info容器 中会独立出 ip地址，mac地址，port 等信息，然后实现网络的共享 完整步骤如下 通过 Pause 容器，把其它业务容器加入到Pause容器里，让所有业务容器在同一个名称空间中，可以实现网络共享 共享存储Pod持久化数据，专门存储到某个地方中 使用 Volumn数据卷进行共享存储，案例如下所示 Pod镜像拉取策略我们以具体实例来说，拉取策略就是 imagePullPolicy 拉取策略主要分为了以下几种 IfNotPresent：默认值，镜像在宿主机上不存在才拉取 Always：每次创建Pod都会重新拉取一次镜像 Never：Pod永远不会主动拉取这个镜像 Pod资源限制也就是我们Pod在进行调度的时候，可以对调度的资源进行限制，例如我们限制 Pod调度是使用的资源是 2C4G，那么在调度对应的node节点时，只会占用对应的资源，对于不满足资源的节点，将不会进行调度 示例我们在下面的地方进行资源的限制 这里分了两个部分 request：表示调度所需的资源 limits：表示最大所占用的资源 Pod重启机制因为Pod中包含了很多个容器，假设某个容器出现问题了，那么就会触发Pod重启机制 重启策略主要分为以下三种 Always：当容器终止退出后，总是重启容器，默认策略 【nginx等，需要不断提供服务】 OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。 Never：当容器终止退出，从不重启容器 【批量任务】 Pod健康检查通过容器检查，原来我们使用下面的命令来检查 1kubectl get pod 但是有的时候，程序可能出现了 Java 堆内存溢出，程序还在运行，但是不能对外提供服务了，这个时候就不能通过 容器检查来判断服务是否可用了 这个时候就可以使用应用层面的检查 12345## 存活检查，如果检查失败，将杀死容器，根据Pod的restartPolicy【重启策略】来操作livenessProbe## 就绪检查，如果检查失败，Kubernetes会把Pod从Service endpoints中剔除readinessProbe Probe支持以下三种检查方式 http Get：发送HTTP请求，返回200 - 400 范围状态码为成功 exec：执行Shell命令返回状态码是0为成功 tcpSocket：发起TCP Socket建立成功 Pod调度策略创建Pod流程 首先创建一个pod，然后创建一个API Server 和 Etcd【把创建出来的信息存储在etcd中】 然后创建 Scheduler，监控API Server是否有新的Pod，如果有的话，会通过调度算法，把pod调度某个node上 在node节点，会通过 kubelet -- apiserver 读取etcd 拿到分配在当前node节点上的pod，然后通过docker创建容器 影响Pod调度的属性Pod资源限制对Pod的调度会有影响 根据request找到足够node节点进行调度 节点选择器标签影响Pod调度 关于节点选择器，其实就是有两个环境，然后环境之间所用的资源配置不同 我们可以通过以下命令，给我们的节点新增标签，然后节点选择器就会进行调度了 1kubectl label node node1 env_role=prod 节点亲和性节点亲和性 nodeAffinity 和 之前nodeSelector 基本一样的，根据节点上标签约束来决定Pod调度到哪些节点上 硬亲和性：约束条件必须满足 软亲和性：尝试满足，不保证 支持常用操作符：in、NotIn、Exists、Gt、Lt、DoesNotExists 反亲和性：就是和亲和性刚刚相反，如 NotIn、DoesNotExists等 污点和污点容忍概述nodeSelector 和 NodeAffinity，都是Prod调度到某些节点上，属于Pod的属性，是在调度的时候实现的。 Taint 污点：节点不做普通分配调度，是节点属性 场景 专用节点【限制ip】 配置特定硬件的节点【固态硬盘】 基于Taint驱逐【在node1不放，在node2放】 查看污点情况1kubectl describe node k8smaster | grep Taint 污点值有三个 NoSchedule：一定不被调度 PreferNoSchedule：尽量不被调度【也有被调度的几率】 NoExecute：不会调度，并且还会驱逐Node已有Pod 未节点添加污点1kubectl taint node [node] key=value:污点的三个值 举例： 1kubectl taint node k8snode1 env_role=yes:NoSchedule 删除污点1kubectl taint node k8snode1 env_role:NoSchedule- 演示我们现在创建多个Pod，查看最后分配到Node上的情况 首先我们创建一个 nginx 的pod 1kubectl create deployment web --image=nginx 然后使用命令查看 1kubectl get pods -o wide 我们可以非常明显的看到，这个Pod已经被分配到 k8snode1 节点上了 下面我们把pod复制5份，在查看情况pod情况 1kubectl scale deployment web --replicas=5 我们可以发现，因为master节点存在污点的情况，所以节点都被分配到了 node1 和 node2节点上 我们可以使用下面命令，把刚刚我们创建的pod都删除 1kubectl delete deployment web 现在给了更好的演示污点的用法，我们现在给 node1节点打上污点 1kubectl taint node k8snode1 env_role=yes:NoSchedule 然后我们查看污点是否成功添加 1kubectl describe node k8snode1 | grep Taint 然后我们在创建一个 pod 1234## 创建nginx podkubectl create deployment web --image=nginx## 复制五次kubectl scale deployment web --replicas=5 然后我们在进行查看 1kubectl get pods -o wide 我们能够看到现在所有的pod都被分配到了 k8snode2上，因为刚刚我们给node1节点设置了污点 最后我们可以删除刚刚添加的污点 1kubectl taint node k8snode1 env_role:NoSchedule- 污点容忍污点容忍就是某个节点可能被调度，也可能不被调度","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes集群YAML文件详解","slug":"K8S/7-Kubernetes集群YAML文件详解","date":"2021-04-23T16:00:00.000Z","updated":"2021-11-26T07:48:35.267Z","comments":true,"path":"2021/04/24/K8S/7-Kubernetes集群YAML文件详解/","link":"","permalink":"https://www.iwhero.com/2021/04/24/K8S/7-Kubernetes%E9%9B%86%E7%BE%A4YAML%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Kubernetes集群YAML文件详解概述k8s 集群中对资源管理和资源对象编排部署都可以通过声明样式（YAML）文件来解决，也就是可以把需要对资源对象操作编辑到YAML 格式文件中，我们把这种文件叫做资源清单文件，通过kubectl 命令直接使用资源清单文件就可以实现对大量的资源对象进行编排部署了。一般在我们开发的时候，都是通过配置YAML文件来部署集群的。 YAML文件：就是资源清单文件，用于资源编排 YAML文件介绍YAML概述YAML ：仍是一种标记语言。为了强调这种语言以数据做为中心，而不是以标记语言为重点。 YAML 是一个可读性高，用来表达数据序列的格式。 YAML 基本语法 使用空格做为缩进 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 低版本缩进时不允许使用Tab 键，只允许使用空格 使用#标识注释，从这个字符一直到行尾，都会被解释器忽略 使用 — 表示新的yaml文件开始 YAML 支持的数据结构对象键值对的集合，又称为映射(mapping) / 哈希（hashes） / 字典（dictionary） 123456## 对象类型：对象的一组键值对，使用冒号结构表示name: Tomage: 18## yaml 也允许另一种写法，将所有键值对写成一个行内对象hash: &#123;name: Tom, age: 18&#125; 数组1234567## 数组类型：一组连词线开头的行，构成一个数组People- Tom- Jack## 数组也可以采用行内表示法People: [Tom, Jack] YAML文件组成部分主要分为了两部分，一个是控制器的定义 和 被控制的对象 控制器的定义 被控制的对象包含一些 镜像，版本、端口等 属性说明在一个YAML文件的控制器定义中，有很多属性名称 属性名称 介绍 apiVersion API版本 kind 资源类型 metadata 资源元数据 spec 资源规格 replicas 副本数量 selector 标签选择器 template Pod模板 metadata Pod元数据 spec Pod规格 containers 容器配置 如何快速编写YAML文件一般来说，我们很少自己手写YAML文件，因为这里面涉及到了很多内容，我们一般都会借助工具来创建 使用kubectl create命令这种方式一般用于资源没有部署的时候，我们可以直接创建一个YAML配置文件 12## 尝试运行,并不会真正的创建镜像kubectl create deployment web --image=nginx -o yaml --dry-run 或者我们可以输出到一个文件中 1kubectl create deployment web --image=nginx -o yaml --dry-run &gt; hello.yaml 然后我们就在文件中直接修改即可 使用kubectl get命令导出yaml文件可以首先查看一个目前已经部署的镜像 1kubectl get deploy 然后我们导出 nginx的配置 1kubectl get deploy nginx -o=yaml --export &gt; nginx.yaml 然后会生成一个 nginx.yaml 的配置文件","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"http和https","slug":"Internet/http与https","date":"2021-04-21T16:00:00.000Z","updated":"2021-04-21T02:51:50.758Z","comments":true,"path":"2021/04/22/Internet/http与https/","link":"","permalink":"https://www.iwhero.com/2021/04/22/Internet/http%E4%B8%8Ehttps/","excerpt":"","text":"http和httpshttphttp是一种无状态协议。无状态是指客户机和服务器之间不需要建立持久连接，这意味着当一个客户端向服务器发出请求，然后服务器返回响应（response），连接就被关闭了，在服务器端不保留连接的有关信息，HTTP遵循请求/应答模型。客户机向服务器发送请求，服务器处理请求并返回适当的应答。所有HTTP连接都构成一套请求和应答。 httpsHTTPS是以安全为目标的HTTP通道，简单将就是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL。其所用的端口是443，过程大致如下： 获取连接证书SSL客户端通过TCP和服务器建立连接后（443端口），并且在一般的TCP连接协商过程中请求证书。即客户端发出一个消息给服务器，这个消息里面包含了自己可实现的算法列表和其它一些需要的消息，SSL的服务器端会回应一个数据包，这里面确定了这次通信所需要的算法，然后服务器向客户端返回证书。（证书里面包含了服务器信息：域名。申请证书的公司，公共密钥） 证书验证客户端在收到服务器返回的证书后，判断签发这个证书的公共签发机构，并使用这个机构的公共密钥确认签名是否有效，客户端还会确保证书中列出的域名就是它正在连接的域名 数据加密和传输如果确认证书有效，那么生成对称密钥并使用服务器的公共密钥进行加密。然后发送给服务器，服务器使用它的密钥进行解密，这样两台计算机可以开始进行对称加密进行通信。 对称加密：是指加密和解密用的都是同一个密钥，目前微信小程序采用的就是这个加密方式 对称加密存在的问题首先我们知道对称加密是指：加密和解密都使用的同一个密钥，这种方式存在的最大的问题就是密钥发送问题，即如果安全的将密钥发送给对方。 为什么叫对称加密？一方通过密钥将信息加密后，把密文传给另一个方，另一方通过这个相同的密钥将密文解密，转换成可以理解的明文。他们之间的关系如下 明文 -&gt; 密钥 -&gt; 密文 但是从上面的图我们可以看出，我们在进行加密后，首先需要将密钥发送给服务器，那么这个过程就可能存在危险的 非对称加密上面提到的是对称加密，其实还有一种是非对称加密，非对称加密是通过两个密钥（公钥 - 私钥）来实现对数据的加密和解密的，公钥用于加密，私钥用于解密。 过程如下： 首先服务器会颁发一个公钥放在网络中，同时它自己还有一份私钥，然后客户端可以直接获取到对应的公钥 然后将客户端的数据进行公钥的加密，加密后传输的服务器中，服务器在进行私钥解密，得到最终的数据 由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性，但是和对称加密比起来，它非常慢，所以我们还是要用对称加密来传送消息，但是对称加密使用的密钥我们通过非对称加密的方式发送出去。这个结果就变成了： 但是我们需要注意的是，此时交换的两个公钥不一定正确，因为可能会被中间人截获，同时掉包 例如：中间人虽然不知道小红的私钥是什么，但是在截获了小红的公钥Key1之后，却可以偷天换日，自己另外生成一对公钥私钥，把自己的公钥Key3发送给小灰。 这一次通信再次被中间人截获，中间人先用自己的私钥解开了Key3的加密，获得Key2，然后再用当初小红发来的Key1重新加密，再发给小红 证书机制这个时候我们需要做的就是从指定的机构出获取公钥，而不是任由其在网络传输 作为服务器端的小红，首先先把自己的公钥给证书颁发机构，向证书颁发机构申请证书 证书颁发机构自己也有一堆公钥和私钥。机构利用自己的私钥来解密Key1，通过服务端网址等信息生成一个证书签名，证书签名同样经过机构的私钥加密。证书制作完成后，机构把证书发送给服务端的小红。 当小灰向小红请求通信的时候，小红不再直接返回自己的公钥，而是把自己申请的证书返回给小灰。 小灰收到证书以后，要做的第一件事就是验证证书的真伪，需要说明的是，各大浏览器和操作系统已经维护了所有权威证书机构的名称和公钥，所以小灰只需要知道是哪个机构颁发的证书，就可以从本地找到对应的机构公钥，解密出证书签名。 参考https://blog.csdn.net/jiangshangchunjiezi/article/details/88545263","categories":[{"name":"网络","slug":"网络","permalink":"https://www.iwhero.com/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://www.iwhero.com/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"三次握手和四次挥手","slug":"Internet/三次握手和四次挥手","date":"2021-04-20T16:00:00.000Z","updated":"2021-04-21T02:50:54.633Z","comments":true,"path":"2021/04/21/Internet/三次握手和四次挥手/","link":"","permalink":"https://www.iwhero.com/2021/04/21/Internet/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","excerpt":"","text":"三次握手和四次挥手三次握手概念为什么需要握手：握手的作用就是为了同步一些信息，比如最大滑动窗口 TCP：是一个可靠的连接，也就是客户端和服务器双方必须感知对方的存在，也就是需要经历一个建立连接的过程 用三次握手建立TCP连接，连接有三个阶段 建立连接 数据传输 连接释放 连接的管理就是使连接的建立和释放都能正常地进行，连接阶段过程中要解决以下三个问题 要使每一方都能确知对方的存在 要允许双方协商一些参数 能够对运输实体分配资源 TCP连接建立过程TCP建立连接的过程：被称为握手 ① 握手过程其实是发送的TCP报文，在这里面有两个字段，SYN 和 seq SYN = 1：表示该报文不能携带数据，但是需要消耗一个SEQ（序号），可以想象成我们对消息编号 seq：TCP的每个字节发送的时候，都有一个序号，主要是为了保证可靠性，比如当我服务器通过TCP报文得到了有N个字节需要接受，但是最后只接受到了N-1个，我们通过序号就知道哪个没有被接收到。 客户端进入SYN_SENT状态，即同步已发送 ② 当服务器接受到我们的握手请求时，会回复一个确认报文 SYN：表示不携带数据，同时消耗一个SEQ = y（这里的y是任意数字，可以是1,2,3,4） ACK：=1 表示这是一条确定报文 ack：x+1，其中x是刚刚客户端发送过来的 服务器进入SYN_RECVD状态，即同步已收到 ③ 当客户端收到确认报文的时候，客户端需要对这个确认报文进行回复 ACK：=1，表示这是一条确认报文 seq：= x +1， ack：= y+1 经过了这三次握手，两者就进入了连接状态 通俗的理解 客户端：服务器，我们可以建立连接么？ -&gt; SYN= 1 ， seq = x 服务器：可以啊，我们建立连接吧 ！ -&gt; ACK =1, SYN = 1, seq = y, ack = x+1 客户端：收到，建立连接吧！ -&gt; ACK = 1, SYN = 1，seq = x + 1， ack = y + 1 然后建立TCP连接 中国机长版三次握手 为什么是三次握手四次握手四次连接有点多余，第三次的时候，我们已经互相进行了连接确认 但是因为我们无法保证百分百的可靠性 两次握手：客户端知道服务器有接收 和 发送的能力，服务器不知道客户端有没有接收数据的能力，因为通过第一次握手，已经知道了客户端能够发送数据，但是能不能接收数据，还是不清楚，因此这个TCP连接是不可靠的。 为什么不能两次握手就建立连接 因为超时重传机制的存在 但客户端发送第一次握手的时候，可能会经历网络拥塞，然后客户端会以为这个连接已经丢失，然后会重新发送一个请求连接的信息到服务器，这次发送的消息很快被服务器接受，然后服务器建立连接就开始建立连接。但是当第一次发送的请求经过一段时间的阻塞后，成功到达服务器，然后服务器又连接连接，而此时客户端是不会理会这次请求的建立，所以服务器一直在等待客户端数据的发送。 四次挥手所谓的四次挥手，就是关闭TCP连接的过程，指的是断开一个TPC连接，需要客户端和服务端总共发送4个包，以确定双方连接的断开。 主要目的：保证TCP连接的全双工连接 四次挥手示意图 由于TCP连接是全双工的，因此每个方向都必须单独关闭，这个原则是当以防完成它的数据发送任务后，就能发送一个FIN包来终止这个方向的连接。 收到一个FIN包只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后，仍然能发送数据，首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 四次挥手过程 第一次挥手：客户端发送一个FIN包（FIN=1，seq=U）给服务器，用来关闭客户端到服务器端的数据传输，客户端进入FIN_WAIT_1状态（终止等待） 第二次挥手：服务器端收到FIN包后，发送一个ACK包（ACK=1，ack=u+1，在随机产生一个值v 给seq）给客户端，服务器进入了CLOSE_WAIT状态（关闭等待） 第三次挥手：服务器端发送一个FIN包（FIN=1，ACK=1，ack=u+1，在随机产生 一个w值给seq）给客户端，用来关闭服务器到客户端的数据传输，服务端进入了LAST_ACK（最后确定）状态 第四次挥手：客户端接收FIN包，然后进入TIME_WAIT状态，接着发送一个ACK包（ACK=1，seq=u+1, ack = w+1） 给服务端，服务端确定序号，进入CLOSe状态，完成了四次挥手。 挥手中的状态 CLOSED：表示初始状态 ESTABLISHED：表示连接已经连接 FIN_WAIT：状态FIN_WAIT_1和FIN_WAIT_2都表示等待对方的FIN报文，这两个状态的区别是，当主动发送方给对方发送了断开请求时，就进入了FIN_WAIT_1状态，而到被动方在回应后，主动发送方就进入了FIN_WAIT_2。 FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你，稍后再关闭连接 CLOSE_WAIT：这个状态的含义是 表示在等待关闭 LAST_ACK：在被动关闭放发送FIN报文后，最后等待对方的ACK报文，当收到了ACK报文后，就进入了CLOSE状态。 为什么TIME_WAIT状态还需要等待2MSL后才能返回CLOSE这是因为虽然双方都同意了关闭连接，而且握手的4个报文也都协调和发送完毕，按道理可以直接回到CLOSE状态 但是因为我们需要假设网络是不可靠的，你无法保证你最后发送的ACK报文是会一定被对方收到，因此处于LAST_ACK状态下的socket可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的报文。 中国机长版四次挥手当客户端与服务器在规定的时间内没有得到应答 会发送报文进行探测，假设没有应答，那么就会关闭连接 下面是四次挥手的过程","categories":[{"name":"网络","slug":"网络","permalink":"https://www.iwhero.com/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://www.iwhero.com/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"Kubernetes集群管理工具kubectl","slug":"K8S/6-Kubernetes集群管理工具kubectl","date":"2021-04-14T16:00:00.000Z","updated":"2021-11-26T07:48:35.289Z","comments":true,"path":"2021/04/15/K8S/6-Kubernetes集群管理工具kubectl/","link":"","permalink":"https://www.iwhero.com/2021/04/15/K8S/6-Kubernetes%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7kubectl/","excerpt":"","text":"Kubernetes集群管理工具kubectl概述kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署 命令格式命令格式如下 1kubectl [command] [type] [name] [flags] 参数 command：指定要对资源执行的操作，例如create、get、describe、delete type：指定资源类型，资源类型是大小写敏感的，开发者能够以单数 、复数 和 缩略的形式 例如： 123kubectl get pod pod1kubectl get pods pod1kubectl get po pod1 name：指定资源的名称，名称也是大小写敏感的，如果省略名称，则会显示所有的资源，例如 1kubectl get pods flags：指定可选的参数，例如，可用 -s 或者 -server参数指定Kubernetes API server的地址和端口 常见命令kubectl help 获取更多信息通过 help命令，能够获取帮助信息 12345## 获取kubectl的命令kubectl --help## 获取某个命令的介绍和使用kubectl get --help 基础命令常见的基础命令 命令 介绍 create 通过文件名或标准输入创建资源 expose 将一个资源公开为一个新的Service run 在集群中运行一个特定的镜像 set 在对象上设置特定的功能 get 显示一个或多个资源 explain 文档参考资料 edit 使用默认的编辑器编辑一个资源 delete 通过文件名，标准输入，资源名称或标签来删除资源 部署命令 命令 介绍 rollout 管理资源的发布 rolling-update 对给定的复制控制器滚动更新 scale 扩容或缩容Pod数量，Deployment、ReplicaSet、RC或Job autoscale 创建一个自动选择扩容或缩容并设置Pod数量 集群管理命令 命令 介绍 certificate 修改证书资源 cluster-info 显示集群信息 top 显示资源(CPU/M) cordon 标记节点不可调度 uncordon 标记节点可被调度 drain 驱逐节点上的应用，准备下线维护 taint 修改节点taint标记 故障和调试命令 命令 介绍 describe 显示特定资源或资源组的详细信息 logs 在一个Pod中打印一个容器日志，如果Pod只有一个容器，容器名称是可选的 attach 附加到一个运行的容器 exec 执行命令到容器 port-forward 转发一个或多个 proxy 运行一个proxy到Kubernetes API Server cp 拷贝文件或目录到容器中 auth 检查授权 其它命令 命令 介绍 apply 通过文件名或标准输入对资源应用配置 patch 使用补丁修改、更新资源的字段 replace 通过文件名或标准输入替换一个资源 convert 不同的API版本之间转换配置文件 label 更新资源上的标签 annotate 更新资源上的注释 completion 用于实现kubectl工具自动补全 api-versions 打印受支持的API版本 config 修改kubeconfig文件（用于访问API，比如配置认证信息） help 所有命令帮助 plugin 运行一个命令行插件 version 打印客户端和服务版本信息 目前使用的命令12345678## 创建一个nginx镜像kubectl create deployment nginx --image=nginx## 对外暴露端口kubectl expose deployment nginx --port=80 --type=NodePort## 查看资源kubectl get pod, svc","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"net core微服务架构","slug":"微服务/net-core微服务架构","date":"2021-04-14T16:00:00.000Z","updated":"2021-11-16T07:06:10.196Z","comments":true,"path":"2021/04/15/微服务/net-core微服务架构/","link":"","permalink":"https://www.iwhero.com/2021/04/15/%E5%BE%AE%E6%9C%8D%E5%8A%A1/net-core%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/","excerpt":"","text":"何为微服务微服务(Microservice)概念据说是在2012年出现，其一出现就对互联网行业产生了巨大影响，因为其理念刚好符合“分而治之”的思想，在日益巨大化的互联网行业内，不免逐步产生了无法把控的思绪混乱，而“微”刚好能解决这个痛点。 微服务的精髓“分而治之”是微服务的精髓！理解了这个精髓，就可以如庖丁解牛般设计你的系统架构。每个相对独立的业务均可拆分为微服务，微服务高度自治，数据、缓存、接口都是自我管理的，不要麻烦别的服务区管理，微服务间的通信一般约定为接口间的通讯和异步消息的通讯。微服务和微服务组合共同提供外部的接口，已形成更大的服务。 微服务的复杂度 由于把许多独立的业务拆成不同的微服务，因此带来的微服务构建的复杂度，一般表现为下列几点： 微服务的注册和发现 微服务的部署和弹性伸缩 微服务间的通讯 微服务间通讯的效率 微服务间的事务性（ACID） 微服务的对外网关、限流熔断 微服务的全局配置 微服务的认证授权（OAuth2） 微服务间的异步通讯、消息 微服务的日志 微服务的监控 .net core 自研落线一 最简单的方案，没有之一：选择原生的asp.net core api方式构建微服务。 微服务的注册和发现 ：集成诸如Zookeeper之类的服务 微服务的部署和弹性伸缩: docker + Kubernetes 微服务间的通讯：Http WebApi 微服务间通讯的效率:低，有级联崩溃的可能性 微服务间的事务性（ACID）:CAP或异步通讯+人工处理 微服务的对外网关、限流熔断:nginx/Kubernetes/IIS/自己轻量包装 微服务的全局配置:DB/Redis/zookeeper 微服务的认证授权（OAuth2）:IdentityServer 微服务间的异步通讯、消息:RabbitMq等类似组件 微服务的日志: log4net /Rabbitmq 微服务的监控：HealthCheck接口 .net core 自研落线二 最简单的方案，没有之一：选择Thrift、Grpc、dotnetty等方式构建微服务。 微服务的注册和发现 ：集成诸如Zookeeper之类的服务 微服务的部署和弹性伸缩: docker + Kubernetes 微服务间的通讯：RPC 微服务间通讯的效率:高 微服务间的事务性（ACID）:CAP或异步通讯+人工处理 微服务的对外网关、限流熔断:nginx/Kubernetes/IIS/自己轻量包装 微服务的全局配置:Apollo等类似相关配置中心 微服务的认证授权（OAuth2）:IdentityServer 微服务间的异步通讯、消息:RabbitMq等类似组件 微服务的日志: 三方组件/自研 微服务的监控：CAT、 KariosDB三方集成 该方案实施起来快速稳定，能满足中小公司到大型公司的过渡。","categories":[{"name":"微服务","slug":"微服务","permalink":"https://www.iwhero.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://www.iwhero.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"Kubeadm和二进制方式对比","slug":"K8S/5-Kubeadm和二进制方式对比","date":"2021-04-06T16:00:00.000Z","updated":"2021-11-26T07:48:35.311Z","comments":true,"path":"2021/04/07/K8S/5-Kubeadm和二进制方式对比/","link":"","permalink":"https://www.iwhero.com/2021/04/07/K8S/5-Kubeadm%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94/","excerpt":"","text":"Kubeadm和二进制方式对比Kubeadm方式搭建K8S集群 安装虚拟机，在虚拟机安装Linux操作系统【3台虚拟机】 对操作系统初始化操作 所有节点安装Docker、kubeadm、kubelet、kubectl【包含master和slave节点】 安装docker、使用yum，不指定版本默认安装最新的docker版本 修改docker仓库地址，yum源地址，改为阿里云地址 安装kubeadm，kubelet 和 kubectl k8s已经发布最新的1.19版本，可以指定版本安装，不指定安装最新版本 yum install -y kubelet kubeadm kubectl 在master节点执行初始化命令操作 kubeadm init 默认拉取镜像地址 K8s.gcr.io国内地址，需要使用国内地址 安装网络插件(CNI) kubectl apply -f kube-flannel.yml 在所有的node节点上，使用join命令，把node添加到master节点上 测试kubernetes集群 二进制方式搭建K8S集群 安装虚拟机和操作系统，对操作系统进行初始化操作 生成cfssl 自签证书 ca-key.pem、ca.pem server-key.pem、server.pem 部署Etcd集群 部署的本质，就是把etcd集群交给 systemd 管理 把生成的证书复制过来，启动，设置开机启动 为apiserver自签证书，生成过程和etcd类似 部署master组件，主要包含以下组件 apiserver controller-manager scheduler 交给systemd管理，并设置开机启动 如果要安装最新的1.19版本，下载二进制文件进行安装 部署node组件 docker kubelet kube-proxy【需要批准kubelet证书申请加入集群】 交给systemd管理组件- 组件启动，设置开机启动 批准kubelet证书申请 并加入集群 部署CNI网络插件 测试Kubernets集群【安装nginx测试】","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"使用二进制方式搭建K8S集群","slug":"K8S/4-使用二进制方式搭建K8S集群","date":"2021-04-04T16:00:00.000Z","updated":"2021-11-26T07:48:35.342Z","comments":true,"path":"2021/04/05/K8S/4-使用二进制方式搭建K8S集群/","link":"","permalink":"https://www.iwhero.com/2021/04/05/K8S/4-%E4%BD%BF%E7%94%A8%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%B9%E5%BC%8F%E6%90%AD%E5%BB%BAK8S%E9%9B%86%E7%BE%A4/","excerpt":"","text":"使用二进制方式搭建K8S集群注意【暂时没有使用二进制方式搭建K8S集群，因此本章节内容不完整… 欢迎小伙伴能补充~】 准备工作在开始之前，部署Kubernetes集群机器需要满足以下几个条件 一台或多台机器，操作系统CentOS 7.x 硬件配置：2GB ，2个CPU，硬盘30GB 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像导入节点 禁止swap分区 步骤 创建多态虚拟机，安装Linux系统 操作系统的初始化 为etcd 和 apiserver 自签证书 部署etcd集群 部署master组件【安装docker、kube-apiserver、kube-controller-manager、kube-scheduler、etcd】 部署node组件【安装kubelet、kube-proxy、docker、etcd】 部署集群网络 准备虚拟机首先我们准备了两台虚拟机，来进行安装测试 主机名 ip k8s_2_master 192.168.177.140 k8s_2_node 192.168.177.141 操作系统的初始化然后我们需要进行一些系列的初始化操作 12345678910111213141516171819202122232425262728293031323334353637383940## 关闭防火墙systemctl stop firewalldsystemctl disable firewalld## 关闭selinux## 永久关闭sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config ## 临时关闭setenforce 0 ## 关闭swap## 临时swapoff -a ## 永久关闭sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab## 根据规划设置主机名【master节点上操作】hostnamectl set-hostname k8s_2_master## 根据规划设置主机名【node1节点操作】hostnamectl set-hostname k8s_2_node1## 在master添加hostscat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.177.140 k8s_2_master192.168.177.141 k8s_2_node1EOF## 将桥接的IPv4流量传递到iptables的链cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF## 生效sysctl --system ## 时间同步yum install ntpdate -yntpdate time.windows.com 部署Etcd集群Etcd是一个分布式键值存储系统，Kubernetes使用Etcd进行数据存储，所以先准备一个Etcd数据库，为了解决Etcd单点故障，应采用集群方式部署，这里使用3台组建集群，可容忍一台机器故障，当然也可以使用5台组件集群，可以容忍2台机器故障 自签证书提到证书，我们想到的就是下面这个情况 这个https证书，其实就是服务器颁发给网站的，代表这是一个安全可信任的网站。 而在我们K8S集群的内部，其实也是有证书的，如果不带证书，那么访问就会受限 同时在集群内部 和 外部的访问，我们也需要签发证书 如果我们使用二进制的方式，那么就需要自己手动签发证书。 自签证书：我们可以想象成在一家公司上班，然后会颁发一个门禁卡，同时一般门禁卡有两种，一个是内部员工的门禁卡，和外部访客门禁卡。这两种门禁卡的权限可能不同，员工的门禁卡可以进入公司的任何地方，而访客的门禁卡是受限的，这个门禁卡其实就是自签证书 准备cfssl证书生成工具cfssl是一个开源的证书管理工具，使用json文件生成证书，相比openssl 更方便使用。找任意一台服务器操作，这里用Master节点。 1234567wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes核心技术Ingress","slug":"K8S/14-Kubernetes核心技术Ingress","date":"2021-03-30T16:00:00.000Z","updated":"2021-11-26T07:48:35.432Z","comments":true,"path":"2021/03/31/K8S/14-Kubernetes核心技术Ingress/","link":"","permalink":"https://www.iwhero.com/2021/03/31/K8S/14-Kubernetes%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AFIngress/","excerpt":"","text":"Kubernetes核心技术Ingress前言原来我们需要将端口号对外暴露，通过 ip + 端口号就可以进行访问 原来是使用Service中的NodePort来实现 在每个节点上都会启动端口 在访问的时候通过任何节点，通过ip + 端口号就能实现访问 但是NodePort还存在一些缺陷 因为端口不能重复，所以每个端口只能使用一次，一个端口对应一个应用 实际访问中都是用域名，根据不同域名跳转到不同端口服务中 Ingress和Pod关系pod 和 ingress 是通过service进行关联的，而ingress作为统一入口，由service关联一组pod中 首先service就是关联我们的pod 然后ingress作为入口，首先需要到service，然后发现一组pod 发现pod后，就可以做负载均衡等操作 Ingress工作流程在实际的访问中，我们都是需要维护很多域名， a.com 和 b.com 然后不同的域名对应的不同的Service，然后service管理不同的pod 需要注意，ingress不是内置的组件，需要我们单独的安装 使用Ingress步骤如下所示 部署ingress Controller【需要下载官方的】 创建ingress规则【对哪个Pod、名称空间配置规则】 创建Nginx Pod创建一个nginx应用，然后对外暴露端口 1234## 创建podkubectl create deployment web --image=nginx## 查看kubectl get pods 对外暴露端口 1kubectl expose deployment web --port=80 --target-port=80 --type:NodePort 部署 ingress controller下面我们来通过yaml的方式，部署我们的ingress，配置文件如下所示 这个文件里面，需要注意的是 hostNetwork: true，改成ture是为了让后面访问到 1kubectl apply -f ingress-con.yaml 通过这种方式，其实我们在外面就能访问，这里还需要在外面添加一层 1kubectl apply -f ingress-con.yaml 最后通过下面命令，查看是否成功部署 ingress 1kubectl get pods -n ingress-nginx 创建ingress规则文件创建ingress规则文件，ingress-h.yaml 添加域名访问规则在windows 的 hosts文件，添加域名访问规则【因为我们没有域名解析，所以只能这样做】 最后通过域名就能访问","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes集群安全机制","slug":"K8S/13-Kubernetes集群安全机制","date":"2021-03-29T16:00:00.000Z","updated":"2021-11-26T07:48:35.456Z","comments":true,"path":"2021/03/30/K8S/13-Kubernetes集群安全机制/","link":"","permalink":"https://www.iwhero.com/2021/03/30/K8S/13-Kubernetes%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Kubernetes集群安全机制概述当我们访问K8S集群时，需要经过三个步骤完成具体操作 认证 鉴权【授权】 准入控制 进行访问的时候，都需要经过 apiserver， apiserver做统一协调，比如门卫 访问过程中，需要证书、token、或者用户名和密码 如果访问pod需要serviceAccount 认证对外不暴露8080端口，只能内部访问，对外使用的端口6443 客户端身份认证常用方式 https证书认证，基于ca证书 http token认证，通过token来识别用户 http基本认证，用户名 + 密码认证 鉴权基于RBAC进行鉴权操作 基于角色访问控制 准入控制就是准入控制器的列表，如果列表有请求内容就通过，没有的话 就拒绝 RBAC介绍基于角色的访问控制，为某个角色设置访问内容，然后用户分配该角色后，就拥有该角色的访问权限 k8s中有默认的几个角色 role：特定命名空间访问权限 ClusterRole：所有命名空间的访问权限 角色绑定 roleBinding：角色绑定到主体 ClusterRoleBinding：集群角色绑定到主体 主体 user：用户 group：用户组 serviceAccount：服务账号 RBAC实现鉴权 创建命名空间 创建命名空间我们可以首先查看已经存在的命名空间 1kubectl get namespace 然后我们创建一个自己的命名空间 roledemo 1kubectl create ns roledemo 命名空间创建Pod为什么要创建命名空间？因为如果不创建命名空间的话，默认是在default下 1kubectl run nginx --image=nginx -n roledemo 创建角色我们通过 rbac-role.yaml进行创建 tip：这个角色只对pod 有 get、list权限 然后通过 yaml创建我们的role 1234## 创建kubectl apply -f rbac-role.yaml## 查看kubectl get role -n roledemo 创建角色绑定我们还是通过 role-rolebinding.yaml 的方式，来创建我们的角色绑定 然后创建我们的角色绑定 1234## 创建角色绑定kubectl apply -f rbac-rolebinding.yaml## 查看角色绑定kubectl get role, rolebinding -n roledemo 使用证书识别身份我们首先得有一个 rbac-user.sh 证书脚本 这里包含了很多证书文件，在TSL目录下，需要复制过来 通过下面命令执行我们的脚本 1./rbac-user.sh 最后我们进行测试 1234## 用get命令查看 pod 【有权限】kubectl get pods -n roledemo## 用get命令查看svc 【没权限】kubectl get svc -n roledmeo","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"使用kubeadm方式搭建K8S集群","slug":"K8S/3-使用kubeadm方式搭建K8S集群","date":"2021-03-29T16:00:00.000Z","updated":"2021-11-26T07:48:35.366Z","comments":true,"path":"2021/03/30/K8S/3-使用kubeadm方式搭建K8S集群/","link":"","permalink":"https://www.iwhero.com/2021/03/30/K8S/3-%E4%BD%BF%E7%94%A8kubeadm%E6%96%B9%E5%BC%8F%E6%90%AD%E5%BB%BAK8S%E9%9B%86%E7%BE%A4/","excerpt":"","text":"使用kubeadm方式搭建K8S集群kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。 这个工具能通过两条指令完成一个kubernetes集群的部署： 12345## 创建一个 Master 节点kubeadm init## 将一个 Node 节点加入到当前集群中kubeadm join &lt;Master节点的IP和端口 &gt; Kubeadm方式搭建K8S集群使用kubeadm方式搭建K8s集群主要分为以下几步 准备三台虚拟机，同时安装操作系统CentOS 7.x 对三个安装之后的操作系统进行初始化操作 在三个节点安装 docker kubelet kubeadm kubectl 在master节点执行kubeadm init命令初始化 在node节点上执行 kubeadm join命令，把node节点添加到当前集群 配置CNI网络插件，用于节点之间的连通【失败了可以多试几次】 通过拉取一个nginx进行测试，能否进行外网测试 安装要求在开始之前，部署Kubernetes集群机器需要满足以下几个条件： 一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多【注意master需要两核】 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点 禁止swap分区 准备环境 角色 IP master 192.168.177.130 node1 192.168.177.131 node2 192.168.177.132 然后开始在每台机器上执行下面的命令 123456789101112131415161718192021222324252627282930313233343536373839404142## 关闭防火墙systemctl stop firewalldsystemctl disable firewalld## 关闭selinux## 永久关闭sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config ## 临时关闭setenforce 0 ## 关闭swap## 临时swapoff -a ## 永久关闭sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab## 根据规划设置主机名【master节点上操作】hostnamectl set-hostname k8smaster## 根据规划设置主机名【node1节点操作】hostnamectl set-hostname k8snode1## 根据规划设置主机名【node2节点操作】hostnamectl set-hostname k8snode2## 在master添加hostscat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.177.130 k8smaster192.168.177.131 k8snode1192.168.177.132 k8snode2EOF## 将桥接的IPv4流量传递到iptables的链cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF## 生效sysctl --system ## 时间同步yum install ntpdate -yntpdate time.windows.com 安装Docker/kubeadm/kubelet所有节点安装Docker/kubeadm/kubelet ，Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker 安装Docker首先配置一下Docker的阿里yum源 12345678cat &gt;/etc/yum.repos.d/docker.repo&lt;&lt;EOF[docker-ce-edge]name=Docker CE Edge - \\$basearchbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/\\$basearch/edgeenabled=1gpgcheck=1gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpgEOF 然后yum方式安装docker 123456789## yum安装yum -y install docker-ce## 查看docker版本docker --version ## 启动dockersystemctl enable dockersystemctl start docker 配置docker的镜像源 12345cat &gt;&gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]&#125;EOF 然后重启docker 1systemctl restart docker 添加kubernetes软件源然后我们还需要配置一下yum的k8s软件源 123456789cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 安装kubeadm，kubelet和kubectl由于版本更新频繁，这里指定版本号部署： 1234## 安装kubelet、kubeadm、kubectl，同时指定版本yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0## 设置开机启动systemctl enable kubelet 部署Kubernetes Master【master节点】在 192.168.177.130 执行，也就是master节点 1kubeadm init --apiserver-advertise-address=192.168.177.130 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.0 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址，【执行上述命令会比较慢，因为后台其实已经在拉取镜像了】，我们 docker images 命令即可查看已经拉取的镜像 当我们出现下面的情况时，表示kubernetes的镜像已经安装成功 使用kubectl工具 【master节点操作】 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 执行完成后，我们使用下面命令，查看我们正在运行的节点 1kubectl get nodes 能够看到，目前有一个master节点已经运行了，但是还处于未准备状态 下面我们还需要在Node节点执行其它的命令，将node1和node2加入到我们的master节点上 加入Kubernetes Node【Slave节点】下面我们需要到 node1 和 node2服务器，执行下面的代码向集群添加新节点 执行在kubeadm init输出的kubeadm join命令： 注意，以下的命令是在master初始化完成后，每个人的都不一样！！！需要复制自己生成的 12kubeadm join 192.168.177.130:6443 --token 8j6ui9.gyr4i156u30y80xf \\ --discovery-token-ca-cert-hash sha256:eda1380256a62d8733f4bddf926f148e57cf9d1a3a58fb45dd6e80768af5a500 默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下： 1kubeadm token create --print-join-command 当我们把两个节点都加入进来后，我们就可以去Master节点 执行下面命令查看情况 1kubectl get node 部署CNI网络插件上面的状态还是NotReady，下面我们需要网络插件，来进行联网访问 12## 下载网络插件配置wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。 1234567891011## 添加kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml##①首先下载v0.13.1-rc2-amd64 镜像##参考博客：https://www.cnblogs.com/pyxuexi/p/14288591.html##② 导入镜像，命令，，特别提示，3个机器都需要导入，3个机器都需要导入，3个机器都需要导入，3个机器都需要导入，重要的事情说3遍。不然抱错。如果没有操作，报错后，需要删除节点，重置，在导入镜像，重新加入才行。本地就是这样操作成功的！docker load &lt; flanneld-v0.13.1-rc2-amd64.docker#####下载本地，替换将image: quay.io/coreos/flannel:v0.13.1-rc2 替换为 image: quay.io/coreos/flannel:v0.13.1-rc2-amd64## 查看状态 【kube-system是k8s中的最小单元】kubectl get pods -n kube-system 运行后的结果 运行完成后，我们查看状态可以发现，已经变成了Ready状态了 如果上述操作完成后，还存在某个节点处于NotReady状态，可以在Master将该节点删除 12345678910## master节点将该节点删除##20210223 yan 查阅资料添加###kubectl drain k8snode1 --delete-local-data --force --ignore-daemonsetskubectl delete node k8snode1 ## 然后到k8snode1节点进行重置 kubeadm reset## 重置完后在加入kubeadm join 192.168.177.130:6443 --token 8j6ui9.gyr4i156u30y80xf --discovery-token-ca-cert-hash sha256:eda1380256a62d8733f4bddf926f148e57cf9d1a3a58fb45dd6e80768af5a500 测试kubernetes集群我们都知道K8S是容器化技术，它可以联网去下载镜像，用容器的方式进行启动 在Kubernetes集群中创建一个pod，验证是否正常运行： 1234## 下载nginx 【会联网拉取nginx镜像】kubectl create deployment nginx --image=nginx## 查看状态kubectl get pod 如果我们出现Running状态的时候，表示已经成功运行了 下面我们就需要将端口暴露出去，让其它外界能够访问 1234## 暴露端口kubectl expose deployment nginx --port=80 --type=NodePort## 查看一下对外的端口kubectl get pod,svc 能够看到，我们已经成功暴露了 80端口 到 30529上 我们到我们的宿主机浏览器上，访问如下地址 1http://192.168.177.130:30529/ 发现我们的nginx已经成功启动了 到这里为止，我们就搭建了一个单master的k8s集群 错误汇总错误一在执行Kubernetes init方法的时候，出现这个问题 12error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2 是因为VMware设置的核数为1，而K8S需要的最低核数应该是2，调整核数重启系统即可 错误二我们在给node1节点使用 kubernetes join命令的时候，出现以下错误 12error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR Swap]: running with swap on is not supported. Please disable swap 错误原因是我们需要关闭swap 12345## 关闭swap## 临时swapoff -a ## 临时sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab 错误三在给node1节点使用 kubernetes join命令的时候，出现以下错误 1The HTTP call equal to &#x27;curl -sSL http://localhost:10248/healthz&#x27; failed with error: Get http://localhost:10248/healthz: dial tcp [::1]:10248: connect: connection refused 解决方法，首先需要到 master 节点，创建一个文件 1234567891011## 创建文件夹mkdir /etc/systemd/system/kubelet.service.d## 创建文件vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf## 添加如下内容Environment=&quot;KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --fail-swap-on=false&quot;## 重置kubeadm reset 然后删除刚刚创建的配置目录 1rm -rf $HOME/.kube 然后 在master重新初始化 1kubeadm init --apiserver-advertise-address=202.193.57.11 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.0 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 初始完成后，我们再到 node1节点，执行 kubeadm join命令，加入到master 12kubeadm join 202.193.57.11:6443 --token c7a7ou.z00fzlb01d76r37s \\ --discovery-token-ca-cert-hash sha256:9c3f3cc3f726c6ff8bdff14e46b1a856e3b8a4cbbe30cab185f6c5ee453aeea5 添加完成后，我们使用下面命令，查看节点是否成功添加 1kubectl get nodes 错误四我们再执行查看节点的时候， kubectl get nodes 会出现问题 1Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;) 这是因为我们之前创建的配置文件还存在，也就是这些配置 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 我们需要做的就是把配置文件删除，然后重新执行一下 1rm -rf $HOME/.kube 然后再次创建一下即可 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 这个问题主要是因为我们在执行 kubeadm reset 的时候，没有把 $HOME/.kube 给移除掉，再次创建时就会出现问题了 错误五安装的时候，出现以下错误 1Another app is currently holding the yum lock; waiting for it to exit... 是因为yum上锁占用，解决方法 1yum -y install docker-ce 错误六在使用下面命令，添加node节点到集群上的时候 1kubeadm join 192.168.177.130:6443 --token jkcz0t.3c40t0bqqz5g8wsb --discovery-token-ca-cert-hash sha256:bc494eeab6b7bac64c0861da16084504626e5a95ba7ede7b9c2dc7571ca4c9e5 然后出现了这个错误 12345678[root@k8smaster ~]## kubeadm join 192.168.177.130:6443 --token jkcz0t.3c40t0bqqz5g8wsb --discovery-token-ca-cert-hash sha256:bc494eeab6b7bac64c0861da16084504626e5a95ba7ede7b9c2dc7571ca4c9e5W1117 06:55:11.220907 11230 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.[preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`To see the stack trace of this error execute with --v=5 or higher 出于安全考虑，Linux系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的ip地址将包发往本机另一网卡，该网卡根据路由表继续发送数据包。这通常就是路由器所要实现的功能。也就是说 /proc/sys/net/ipv4/ip_forward 文件的值不支持转发 0：禁止 1：转发 所以我们需要将值修改成1即可 1echo “1” &gt; /proc/sys/net/ipv4/ip_forward 修改完成后，重新执行命令即可","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"Kubernetes简介","slug":"K8S/1-Kubernetes简介","date":"2021-03-24T16:00:00.000Z","updated":"2021-11-26T07:48:35.564Z","comments":true,"path":"2021/03/25/K8S/1-Kubernetes简介/","link":"","permalink":"https://www.iwhero.com/2021/03/25/K8S/1-Kubernetes%E7%AE%80%E4%BB%8B/","excerpt":"","text":"Kubernetes简介来源bilibili尚硅谷K8S视频：https:/www.bilibili.com/video/BV1GT4y1A756 中文官网：https:/kubernetes.io/zh 中文社区：https:/www.kubernetes.org.cn/ 介绍K8S主要讲的就是Kubernetes，首先Kubernetes首字母为K，末尾为s，中间一共有8个字母，所以简称K8s 前置知识 Linux操作系统 Docker 课程简介 K8s概念和架构 从零搭建K8s集群 基于客户端工具kubeadm搭建（简单，最多半小时） 基于二进制包方式（能看到内部的架构） K8s核心概念 Pod：K8s管理的最小单位级，是所有业务类型的基础 Controller：控制器，有状态，无状态，一次任务，定时任务，守护进程 Service Ingress：对外暴露端口 RBAC：安全机制，权限模型 Helm：下载机制 持久化存储 搭建集群监控平台系统 从零搭建高可用K8s集群 在集群环境部署项目 K8S概念和特性部署发展历程我们的项目部署也在经历下面的这样一个历程 传统部署 -&gt; 虚拟化部署时代 -&gt; 容器部署时代 传统部署时代：早期，组织在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。例如，如果在物理服务器上运行多个应用程序，则可能会出现-一个应用程序占用大部分资源的情况，结果可能导致其他应用程序的性能下降。–种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展，并且组织维护许多物理服务器的成本很高。 虚拟化部署时代：作为解决方案，引入了虚拟化功能，它允许您在单个物理服务器的CPU.上运行多个虚拟机（VM）。虚拟化功能允许应用程序在VM之间隔离，并提供安全级别，因为一一个应用程序的信息不能被另一应用程序自由地访问。因为虚拟化可以轻松地添加或更新应用程序、降低硬件成本等等，所以虚拟化可以更好地利用物理服务器中的资源，并可以实现更好的可伸缩性。每个VM是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。 容器部署时代：容器类似于VM，但是它们具有轻量级的隔离属性，可以在应用程序之间共享操作系统（OS），因此，容器被认为是轻量级的。容器与VM类似，具有自己的文件系统、CPU、内存、进程空间等。由于它们与基础架构分离，因此可以跨云和OS分发进行移植。 容器因具有许多优势而变得流行起来。下面列出了容器的一些好处： 敏捷应用程序的创建和部署：与使用VM镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过简单的回滚（由于镜像不可变性），提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建/时而不是在部署时创建应用程序容器镜像，将应用程序与基础架构分离。 可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 云和操作系统分发的可移植性：可在Ubuntu、RHEL、RHEL、CoreOS、本地、Google Kubernetes Engine和其它任何其它地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行OS到使用逻辑资源在OS上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理-而不是在一台大型单机上器体运行。 资源隔离：可预测的应用程序性能。 K8S概述kubernetes，简称K8s，是用8 代替8 个字符“ubernete”而成的缩写。是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes 提供了应用部署，规划，更新，维护的一种机制。 传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新/回滚等操作，当然也可以通过创建虚拟机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。 新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。相对于虚拟机，容器能快速部署，由于容器与底层设施、机器文件系统解耦的。 总结： K8s是谷歌在2014年发布的容器化集群管理系统 使用k8s进行容器化应用部署 使用k8s利于应用扩展 k8s目标实施让部署容器化应用更加简洁和高效 K8S概述Kubernetes 是一个轻便的和可扩展的开源平台，用于管理容器化应用和服务。通过Kubernetes 能够进行应用的自动化部署和扩缩容。在Kubernetes 中，会将组成应用的容器组合成一个逻辑单元以更易管理和发现。 Kubernetes 积累了作为Google 生产环境运行工作负载15 年的经验，并吸收了来自于社区的最佳想法和实践。 K8S功能自动装箱基于容器对应用运行环境的资源配置要求自动部署应用容器 自我修复(自愈能力)当容器失败时，会对容器进行重启 当所部署的Node节点有问题时，会对容器进行重新部署和重新调度 当容器未通过监控检查时，会关闭此容器直到容器正常运行时，才会对外提供服务 如果某个服务器上的应用不响应了，Kubernetes会自动在其它的地方创建一个 水平扩展通过简单的命令、用户UI 界面或基于CPU 等资源使用情况，对应用容器进行规模扩大或规模剪裁 当我们有大量的请求来临时，我们可以增加副本数量，从而达到水平扩展的效果 当黄色应用过度忙碌，会来扩展一个应用 服务发现用户不需使用额外的服务发现机制，就能够基于Kubernetes 自身能力实现服务发现和负载均衡 对外提供统一的入口，让它来做节点的调度和负载均衡， 相当于微服务里面的网关？ 滚动更新可以根据应用的变化，对应用容器运行的应用，进行一次性或批量式更新 添加应用的时候，不是加进去就马上可以进行使用，而是需要判断这个添加进去的应用是否能够正常使用 版本回退可以根据应用部署情况，对应用容器运行的应用，进行历史版本即时回退 类似于Git中的回滚 密钥和配置管理在不需要重新构建镜像的情况下，可以部署和更新密钥和应用配置，类似热部署。 存储编排自动实现存储系统挂载及应用，特别对有状态应用实现数据持久化非常重要 存储系统可以来自于本地目录、网络存储(NFS、Gluster、Ceph 等)、公共云存储服务 批处理提供一次性任务，定时任务；满足批量数据处理和分析的场景 K8S架构组件完整架构图 架构细节K8S架构主要包含两部分：Master（主控节点）和 node（工作节点） master节点架构图 Node节点架构图 k8s 集群控制节点，对集群进行调度管理，接受集群外用户去集群操作请求； master：主控节点 API Server：集群统一入口，以restful风格进行操作，同时交给etcd存储 提供认证、授权、访问控制、API注册和发现等机制 scheduler：节点的调度，选择node节点应用部署 controller-manager：处理集群中常规后台任务，一个资源对应一个控制器 etcd：存储系统，用于保存集群中的相关数据 Work node：工作节点 Kubelet：master派到node节点代表，管理本机容器 一个集群中每个节点上运行的代理，它保证容器都运行在Pod中 负责维护容器的生命周期，同时也负责Volume(CSI) 和 网络(CNI)的管理 kube-proxy：提供网络代理，负载均衡等操作 容器运行环境【Container Runtime】 容器运行环境是负责运行容器的软件 Kubernetes支持多个容器运行环境：Docker、containerd、cri-o、rktlet以及任何实现Kubernetes CRI (容器运行环境接口) 的软件。 fluentd：是一个守护进程，它有助于提升 集群层面日志 K8S核心概念Pod Pod是K8s中最小的单元 一组容器的集合 共享网络【一个Pod中的所有容器共享同一网络】 生命周期是短暂的（服务器重启后，就找不到了） Volume 声明在Pod容器中可访问的文件目录 可以被挂载到Pod中一个或多个容器指定路径下 支持多种后端存储抽象【本地存储、分布式存储、云存储】 Controller 确保预期的pod副本数量【ReplicaSet】 无状态应用部署【Depoltment】 无状态就是指，不需要依赖于网络或者ip 有状态应用部署【StatefulSet】 有状态需要特定的条件 确保所有的node运行同一个pod 【DaemonSet】 一次性任务和定时任务【Job和CronJob】 Deployment 定义一组Pod副本数目，版本等 通过控制器【Controller】维持Pod数目【自动回复失败的Pod】 通过控制器以指定的策略控制版本【滚动升级、回滚等】 Service 定义一组pod的访问规则 Pod的负载均衡，提供一个或多个Pod的稳定访问地址 支持多种方式【ClusterIP、NodePort、LoadBalancer】 可以用来组合pod，同时对外提供服务 Labellabel：标签，用于对象资源查询，筛选 Namespace命名空间，逻辑隔离 一个集群内部的逻辑隔离机制【鉴权、资源】 每个资源都属于一个namespace 同一个namespace所有资源不能重复 不同namespace可以资源名重复 API我们通过Kubernetes的API来操作整个集群 同时我们可以通过 kubectl 、ui、curl 最终发送 http + json/yaml 方式的请求给API Server，然后控制整个K8S集群，K8S中所有的资源对象都可以采用 yaml 或 json 格式的文件定义或描述 如下：使用yaml部署一个nginx的pod 完整流程 通过Kubectl提交一个创建RC（Replication Controller）的请求，该请求通过APlserver写入etcd 此时Controller Manager通过API Server的监听资源变化的接口监听到此RC事件 分析之后，发现当前集群中还没有它所对应的Pod实例 于是根据RC里的Pod模板定义一个生成Pod对象，通过APIServer写入etcd 此事件被Scheduler发现，它立即执行执行一个复杂的调度流程，为这个新的Pod选定一个落户的Node，然后通过API Server讲这一结果写入etcd中 目标Node上运行的Kubelet进程通过APiserver监测到这个”新生的Pod.并按照它的定义，启动该Pod并任劳任怨地负责它的下半生，直到Pod的生命结束 随后，我们通过Kubectl提交一个新的映射到该Pod的Service的创建请求 ControllerManager通过Label标签查询到关联的Pod实例，然后生成Service的Endpoints信息，并通过APIServer写入到etod中， 接下来，所有Node上运行的Proxy进程通过APIServer查询并监听Service对象与其对应的Endponts信息，建立一个软件方式的负载均衡器来实现Service访问到后端Pod的流量转发功能","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"搭建K8S集群前置知识","slug":"K8S/2-搭建K8S集群前置知识","date":"2021-03-22T16:00:00.000Z","updated":"2021-11-26T07:48:35.409Z","comments":true,"path":"2021/03/23/K8S/2-搭建K8S集群前置知识/","link":"","permalink":"https://www.iwhero.com/2021/03/23/K8S/2-%E6%90%AD%E5%BB%BAK8S%E9%9B%86%E7%BE%A4%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/","excerpt":"","text":"搭建K8S集群搭建k8s环境平台规划单master集群单个master节点，然后管理多个node节点 多master集群多个master节点，管理多个node节点，同时中间多了一个负载均衡的过程 服务器硬件配置要求测试环境master：2核 4G 20G node： 4核 8G 40G 生产环境master：8核 16G 100G node： 16核 64G 200G 目前生产部署Kubernetes集群主要有两种方式 kubeadmkubeadm是一个K8S部署工具，提供kubeadm init 和 kubeadm join，用于快速部署Kubernetes集群 官网地址：点我传送 二进制包从github下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群。 Kubeadm降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。 Kubeadm部署集群kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署： 创建一个Master 节点kubeadm init 将Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的IP 和端口&gt; 安装要求在开始之前，部署Kubernetes集群机器需要满足以下几个条件 一台或多台机器，操作系统为Centos7.X 硬件配置：2GB或更多GAM，2个CPU或更多CPU，硬盘30G 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap分区","categories":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"}]},{"title":"MarkDown语法","slug":"Hexo/我的第一篇文章","date":"2021-03-06T10:07:18.000Z","updated":"2021-04-21T01:46:44.497Z","comments":true,"path":"2021/03/06/Hexo/我的第一篇文章/","link":"","permalink":"https://www.iwhero.com/2021/03/06/Hexo/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","excerpt":"","text":"标题使用标题时# 段落这个第一个段落内容 这是第二个段落内容 区块引用 区块引用 超链接百度 图片 无序列表 无序列表 无序列表 无序列表有序列表 有序列表 有序列表分割线","categories":[{"name":"hexo","slug":"hexo","permalink":"https://www.iwhero.com/categories/hexo/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://www.iwhero.com/tags/markdown/"}]}],"categories":[{"name":"xunit","slug":"xunit","permalink":"https://www.iwhero.com/categories/xunit/"},{"name":"设计模式","slug":"设计模式","permalink":"https://www.iwhero.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/categories/k8s/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://www.iwhero.com/categories/elasticsearch/"},{"name":"docker","slug":"docker","permalink":"https://www.iwhero.com/categories/docker/"},{"name":"kibana","slug":"kibana","permalink":"https://www.iwhero.com/categories/kibana/"},{"name":"net","slug":"net","permalink":"https://www.iwhero.com/categories/net/"},{"name":"网络","slug":"网络","permalink":"https://www.iwhero.com/categories/%E7%BD%91%E7%BB%9C/"},{"name":"微服务","slug":"微服务","permalink":"https://www.iwhero.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"hexo","slug":"hexo","permalink":"https://www.iwhero.com/categories/hexo/"}],"tags":[{"name":"xunit","slug":"xunit","permalink":"https://www.iwhero.com/tags/xunit/"},{"name":"redis","slug":"redis","permalink":"https://www.iwhero.com/tags/redis/"},{"name":"设计模式","slug":"设计模式","permalink":"https://www.iwhero.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"k8s","slug":"k8s","permalink":"https://www.iwhero.com/tags/k8s/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://www.iwhero.com/tags/elasticsearch/"},{"name":"docker","slug":"docker","permalink":"https://www.iwhero.com/tags/docker/"},{"name":"Kibana","slug":"Kibana","permalink":"https://www.iwhero.com/tags/Kibana/"},{"name":"网络","slug":"网络","permalink":"https://www.iwhero.com/tags/%E7%BD%91%E7%BB%9C/"},{"name":"微服务","slug":"微服务","permalink":"https://www.iwhero.com/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"markdown","slug":"markdown","permalink":"https://www.iwhero.com/tags/markdown/"}]}